{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "specialized-zambia",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, random_split\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import BertTokenizer, get_linear_schedule_with_warmup\n",
    "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "front-experiment",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import time\n",
    "import datetime\n",
    "import random, re\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "white-vermont",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are %d GPU(s) available. 2\n",
      "We will use the GPU: GeForce RTX 2080 Ti\n"
     ]
    }
   ],
   "source": [
    "# Check device \n",
    "# Get the GPU device name if available.\n",
    "if torch.cuda.is_available():    \n",
    "\n",
    "    # Tell PyTorch to use the GPU.    \n",
    "    device = torch.device(\"cuda\")\n",
    "\n",
    "    print('There are %d GPU(s) available. {}'.format(torch.cuda.device_count()))\n",
    "\n",
    "    print('We will use the GPU: {}'.format(torch.cuda.get_device_name(0)))\n",
    "\n",
    "# If we dont have GPU but a CPU, training will take place on CPU instead\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")\n",
    "    \n",
    "# Set the seed value all over the place to make this reproducible.\n",
    "seed_val = 42\n",
    "\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collective-casting",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!git clone https://github.com/mfliou/nlp_covid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "billion-bones",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 6936 entries, 0 to 6935\n",
      "Data columns (total 3 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   Id      6936 non-null   int64 \n",
      " 1   Text    6936 non-null   object\n",
      " 2   Label   6936 non-null   int64 \n",
      "dtypes: int64(2), object(1)\n",
      "memory usage: 162.7+ KB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000 entries, 0 to 999\n",
      "Data columns (total 3 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   Id      1000 non-null   int64 \n",
      " 1   Text    1000 non-null   object\n",
      " 2   Label   1000 non-null   int64 \n",
      "dtypes: int64(2), object(1)\n",
      "memory usage: 23.6+ KB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2000 entries, 0 to 1999\n",
      "Data columns (total 3 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   Id      2000 non-null   int64 \n",
      " 1   Text    2000 non-null   object\n",
      " 2   Label   2000 non-null   int64 \n",
      "dtypes: int64(2), object(1)\n",
      "memory usage: 47.0+ KB\n",
      "None\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Text</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1241490299215634434</td>\n",
       "      <td>Official death toll from #covid19 in the United Kingdom is now GREATER than: Germany + Poland + Switzerland + Austria + Portugal + Greece + Sweden...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1245916400981381130</td>\n",
       "      <td>Dearest Mr. President @USER 1,169 coronavirus deaths in the US in 24 hours (?) Covid19 pandemic is an international crime from China - not a natur...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1241132432402849793</td>\n",
       "      <td>Latest Updates March 20 ‚ö†Ô∏è5274 new cases and 38 new deaths in the United States Illinois: Governo Pritzker issues \"stay at home\" order for all res...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1236107253666607104</td>\n",
       "      <td>ÁúüÊääÂÖ¨‰∏ª‰∏çÂΩìÂπ≤ÈÉ® BREAKING: 21 people on Grand Princess cruise ship docked off the California coast tested positive for coronavirus, including 19 crew memb...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1239673817552879619</td>\n",
       "      <td>OKLAHOMA CITY ‚Äî The State Department of Education announced Monday the closure of all K-12 public schools statewide until at least April 6 as the ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Id  \\\n",
       "0  1241490299215634434   \n",
       "1  1245916400981381130   \n",
       "2  1241132432402849793   \n",
       "3  1236107253666607104   \n",
       "4  1239673817552879619   \n",
       "\n",
       "                                                                                                                                                    Text  \\\n",
       "0  Official death toll from #covid19 in the United Kingdom is now GREATER than: Germany + Poland + Switzerland + Austria + Portugal + Greece + Sweden...   \n",
       "1  Dearest Mr. President @USER 1,169 coronavirus deaths in the US in 24 hours (?) Covid19 pandemic is an international crime from China - not a natur...   \n",
       "2  Latest Updates March 20 ‚ö†Ô∏è5274 new cases and 38 new deaths in the United States Illinois: Governo Pritzker issues \"stay at home\" order for all res...   \n",
       "3  ÁúüÊääÂÖ¨‰∏ª‰∏çÂΩìÂπ≤ÈÉ® BREAKING: 21 people on Grand Princess cruise ship docked off the California coast tested positive for coronavirus, including 19 crew memb...   \n",
       "4  OKLAHOMA CITY ‚Äî The State Department of Education announced Monday the closure of all K-12 public schools statewide until at least April 6 as the ...   \n",
       "\n",
       "   Label  \n",
       "0      1  \n",
       "1      1  \n",
       "2      1  \n",
       "3      1  \n",
       "4      0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the data\n",
    "train = pd.read_csv('./nlp_covid/data/covid_train.tsv',sep='\\t')\n",
    "train = train.rename(columns={'Label': 'pre-Label'})\n",
    "\n",
    "valid = pd.read_csv('./nlp_covid/data/covid_valid.tsv', sep='\\t', names=['Id', 'Text', 'pre-Label'])\n",
    "\n",
    "test = pd.read_csv('./nlp_covid/data/covid_test.tsv',sep='\\t', names=['Id', 'Text', 'pre-Label'])\n",
    "\n",
    "train['Label'] = train['pre-Label'].apply(lambda x: 1 if x == 'INFORMATIVE' else 0)\n",
    "valid['Label'] = valid['pre-Label'].apply(lambda x: 1 if x == 'INFORMATIVE' else 0)\n",
    "test['Label'] = test['pre-Label'].apply(lambda x: 1 if x == 'INFORMATIVE' else 0)\n",
    "\n",
    "train = train.drop('pre-Label', axis = 1)\n",
    "test = test.drop('pre-Label', axis = 1)\n",
    "valid = valid.drop('pre-Label', axis = 1)\n",
    "\n",
    "pd.set_option('display.max_colwidth', 150)\n",
    "\n",
    "\n",
    "print(train.info())\n",
    "print(valid.info())\n",
    "print(test.info())\n",
    "print()\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "interracial-organization",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Text</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1241728922192142336</td>\n",
       "      <td>For those saying Pakistan isn‚Äôt Italy; After 3 weeks of the first confirmed case, Pakistan has a higher #Coronavirus growth rate than Italy. Exper...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1235713405992030209</td>\n",
       "      <td>Second case DR üá©üá¥ The Canadian woman has not been identified, however they indicated that she is 70 years old and that she was staying with her hu...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1245941302367305728</td>\n",
       "      <td>Kill Chain: the cyber war on America's elections is a MUST SEE documentary for everyone who uses computers. HTTPURL As of 4/3/20 @USER @USER will ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1245913002840391681</td>\n",
       "      <td>Town hosts FIRST #Virtual #TownCouncil meeting via @USER This technology was implemented due to #socialdistancing orders and FL EO 20-69 allowing ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1240543259299987457</td>\n",
       "      <td>Report suggested that the actual number of undiagnosed Coronavirus positive cases in the country may be five 20 times than the official figure at ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Id  \\\n",
       "0  1241728922192142336   \n",
       "1  1235713405992030209   \n",
       "2  1245941302367305728   \n",
       "3  1245913002840391681   \n",
       "4  1240543259299987457   \n",
       "\n",
       "                                                                                                                                                    Text  \\\n",
       "0  For those saying Pakistan isn‚Äôt Italy; After 3 weeks of the first confirmed case, Pakistan has a higher #Coronavirus growth rate than Italy. Exper...   \n",
       "1  Second case DR üá©üá¥ The Canadian woman has not been identified, however they indicated that she is 70 years old and that she was staying with her hu...   \n",
       "2  Kill Chain: the cyber war on America's elections is a MUST SEE documentary for everyone who uses computers. HTTPURL As of 4/3/20 @USER @USER will ...   \n",
       "3  Town hosts FIRST #Virtual #TownCouncil meeting via @USER This technology was implemented due to #socialdistancing orders and FL EO 20-69 allowing ...   \n",
       "4  Report suggested that the actual number of undiagnosed Coronavirus positive cases in the country may be five 20 times than the official figure at ...   \n",
       "\n",
       "   Label  \n",
       "0      0  \n",
       "1      1  \n",
       "2      0  \n",
       "3      0  \n",
       "4      0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "quick-attendance",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Text</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1235770448966754309</td>\n",
       "      <td>@USER PA hospitals don‚Äôt have the capacity. Latest from our local hospital is they will only test those with symptoms who returned from China, Ita...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1235748200742416384</td>\n",
       "      <td>Coronavirus outbreak: Intel employee in Bengaluru may be discharged today. Follow LIVE updates here: HTTPURL</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1236129620363100161</td>\n",
       "      <td>Trump is trying to BS his way through the coronavirus outbreak. ‚Å¶@USER brings together good evidence and comparisons here that suggest Trump is mo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1241191765195001857</td>\n",
       "      <td>@USER How dramatically could we slow down this #Coronavirus from spreading if literally every american wore a mask for 2-3 weeks? I think it would...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1245182515213676546</td>\n",
       "      <td>An Instagram post from comedienne Amy Schumer showed noisemaking and applause coming from buildings at around 7 pm, which is when shift changes oc...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Id  \\\n",
       "0  1235770448966754309   \n",
       "1  1235748200742416384   \n",
       "2  1236129620363100161   \n",
       "3  1241191765195001857   \n",
       "4  1245182515213676546   \n",
       "\n",
       "                                                                                                                                                    Text  \\\n",
       "0  @USER PA hospitals don‚Äôt have the capacity. Latest from our local hospital is they will only test those with symptoms who returned from China, Ita...   \n",
       "1                                           Coronavirus outbreak: Intel employee in Bengaluru may be discharged today. Follow LIVE updates here: HTTPURL   \n",
       "2  Trump is trying to BS his way through the coronavirus outbreak. ‚Å¶@USER brings together good evidence and comparisons here that suggest Trump is mo...   \n",
       "3  @USER How dramatically could we slow down this #Coronavirus from spreading if literally every american wore a mask for 2-3 weeks? I think it would...   \n",
       "4  An Instagram post from comedienne Amy Schumer showed noisemaking and applause coming from buildings at around 7 pm, which is when shift changes oc...   \n",
       "\n",
       "   Label  \n",
       "0      0  \n",
       "1      1  \n",
       "2      0  \n",
       "3      0  \n",
       "4      0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "systematic-publisher",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training sentences: 6,936\n",
      "\n",
      "Number of validation sentences: 1,000\n",
      "\n",
      "Number of test sentences: 2,000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Report the number of sentences.\n",
    "print('Number of training sentences: {:,}\\n'.format(train.shape[0]))\n",
    "print('Number of validation sentences: {:,}\\n'.format(valid.shape[0]))\n",
    "print('Number of test sentences: {:,}\\n'.format(test.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "olive-starter",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Text</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1241490299215634434</td>\n",
       "      <td>Official death toll from #covid19 in the United Kingdom is now GREATER than: Germany + Poland + Switzerland + Austria + Portugal + Greece + Sweden...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1245916400981381130</td>\n",
       "      <td>Dearest Mr. President @USER 1,169 coronavirus deaths in the US in 24 hours (?) Covid19 pandemic is an international crime from China - not a natur...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1241132432402849793</td>\n",
       "      <td>Latest Updates March 20 ‚ö†Ô∏è5274 new cases and 38 new deaths in the United States Illinois: Governo Pritzker issues \"stay at home\" order for all res...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1236107253666607104</td>\n",
       "      <td>ÁúüÊääÂÖ¨‰∏ª‰∏çÂΩìÂπ≤ÈÉ® BREAKING: 21 people on Grand Princess cruise ship docked off the California coast tested positive for coronavirus, including 19 crew memb...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1239673817552879619</td>\n",
       "      <td>OKLAHOMA CITY ‚Äî The State Department of Education announced Monday the closure of all K-12 public schools statewide until at least April 6 as the ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Id  \\\n",
       "0  1241490299215634434   \n",
       "1  1245916400981381130   \n",
       "2  1241132432402849793   \n",
       "3  1236107253666607104   \n",
       "4  1239673817552879619   \n",
       "\n",
       "                                                                                                                                                    Text  \\\n",
       "0  Official death toll from #covid19 in the United Kingdom is now GREATER than: Germany + Poland + Switzerland + Austria + Portugal + Greece + Sweden...   \n",
       "1  Dearest Mr. President @USER 1,169 coronavirus deaths in the US in 24 hours (?) Covid19 pandemic is an international crime from China - not a natur...   \n",
       "2  Latest Updates March 20 ‚ö†Ô∏è5274 new cases and 38 new deaths in the United States Illinois: Governo Pritzker issues \"stay at home\" order for all res...   \n",
       "3  ÁúüÊääÂÖ¨‰∏ª‰∏çÂΩìÂπ≤ÈÉ® BREAKING: 21 people on Grand Princess cruise ship docked off the California coast tested positive for coronavirus, including 19 crew memb...   \n",
       "4  OKLAHOMA CITY ‚Äî The State Department of Education announced Monday the closure of all K-12 public schools statewide until at least April 6 as the ...   \n",
       "\n",
       "   Label  \n",
       "0      1  \n",
       "1      1  \n",
       "2      1  \n",
       "3      1  \n",
       "4      0  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def clean_text(text):\n",
    "    \n",
    "    text = text.lower() #lowercase\n",
    "    \n",
    "    text = re.sub(r'[!]+','!',text)\n",
    "    text = re.sub(r'[?]+','?',text)\n",
    "    text = re.sub(r'[.]+','.',text)\n",
    "    text = re.sub(r\"'\",\"\",text)\n",
    "    text = re.sub('\\s+', '', text).strip() # Remove and double spaces\n",
    "    text = re.sub(r'&amp;?',r'and', text) # replace & -> and\n",
    "    text = re.sub(r\"https?:\\/\\/t.co\\/[A-Za-z0-9]+\", \"\", text) # Remove URLs\n",
    "    text = re.sub(r'[:\"$%&\\*+,-/:;<=>@\\\\^_`{|}~]+','',text) #remove some puncts (except . ! # ?)\n",
    "    \n",
    "    return text\n",
    "\n",
    "#train['text'] = train['text'].apply(clean_text)\n",
    "#test['text'] = test['text'].apply(clean_text)\n",
    "\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "remarkable-missouri",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of informative and uninformative tweets contained in the train dataset:\n",
      "0    3663\n",
      "1    3273\n",
      "Name: Label, dtype: int64\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of informative and uninformative tweets contained in the valid dataset:\n",
      "0    528\n",
      "1    472\n",
      "Name: Label, dtype: int64\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of informative and uninformative tweets contained in the test dataset:\n",
      "0    1056\n",
      "1     944\n",
      "Name: Label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Plot count of true and false tweets in the train set\n",
    "sns.countplot(x=\"Label\", data=train, palette=\"dark\", linewidth=5)\n",
    "plt.show()\n",
    "\n",
    "print('Number of informative and uninformative tweets contained in the train dataset:')\n",
    "print(train['Label'].value_counts())\n",
    "\n",
    "sns.countplot(x=\"Label\", data=valid, palette=\"dark\", linewidth=5)\n",
    "plt.show()\n",
    "\n",
    "print('Number of informative and uninformative tweets contained in the valid dataset:')\n",
    "print(valid['Label'].value_counts())\n",
    "\n",
    "sns.countplot(x=\"Label\", data=test, palette=\"dark\", linewidth=5)\n",
    "plt.show()\n",
    "\n",
    "print('Number of informative and uninformative tweets contained in the test dataset:')\n",
    "print(test['Label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fatty-deputy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the lists of sentences and their labels.\n",
    "train_sentences = train.Text.values\n",
    "train_labels = train.Label.values\n",
    "\n",
    "valid_sentences = valid.Text.values\n",
    "valid_labels = valid.Label.values\n",
    "\n",
    "test_sentences = test.Text.values\n",
    "test_labels = test.Label.values\n",
    "\n",
    "# Load the BERT tokenizer.\n",
    "#To feed our text to BERT, it must be split into tokens, and then these tokens must be \n",
    "#mapped to their index in the tokenizer vocabulary.\n",
    "#The tokenization must be performed by the tokenizer included with BERT\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "massive-creature",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Original:  Official death toll from #covid19 in the United Kingdom is now GREATER than: Germany + Poland + Switzerland + Austria + Portugal + Greece + Sweden + Finland + Norway + Ireland... COMBINED. UK: 67.5 Million (233 dead) Above group: 185 Million (230 dead) HTTPURL\n",
      "valid Original:  For those saying Pakistan isn‚Äôt Italy; After 3 weeks of the first confirmed case, Pakistan has a higher #Coronavirus growth rate than Italy. Experts on the issue say Italy was too slow to lockdown. Even after lockdown, it took time for it to properly have an impact. #Corona\n",
      "test Original:  @USER PA hospitals don‚Äôt have the capacity. Latest from our local hospital is they will only test those with symptoms who returned from China, Italy, Iran, Japan , or South Korea in last 14 days or those who have been in contact with someone with confirmed COVID-19 in the last 14 days.\n",
      "Tokenized train:  ['official', 'death', 'toll', 'from', '#', 'co', '##vid', '##19', 'in', 'the', 'united', 'kingdom', 'is', 'now', 'greater', 'than', ':', 'germany', '+', 'poland', '+', 'switzerland', '+', 'austria', '+', 'portugal', '+', 'greece', '+', 'sweden', '+', 'finland', '+', 'norway', '+', 'ireland', '.', '.', '.', 'combined', '.', 'uk', ':', '67', '.', '5', 'million', '(', '233', 'dead', ')', 'above', 'group', ':', '185', 'million', '(', '230', 'dead', ')', 'http', '##ur', '##l']\n",
      "Tokenized valid:  ['for', 'those', 'saying', 'pakistan', 'isn', '‚Äô', 't', 'italy', ';', 'after', '3', 'weeks', 'of', 'the', 'first', 'confirmed', 'case', ',', 'pakistan', 'has', 'a', 'higher', '#', 'corona', '##virus', 'growth', 'rate', 'than', 'italy', '.', 'experts', 'on', 'the', 'issue', 'say', 'italy', 'was', 'too', 'slow', 'to', 'lock', '##down', '.', 'even', 'after', 'lock', '##down', ',', 'it', 'took', 'time', 'for', 'it', 'to', 'properly', 'have', 'an', 'impact', '.', '#', 'corona']\n",
      "Tokenized test:  ['@', 'user', 'pa', 'hospitals', 'don', '‚Äô', 't', 'have', 'the', 'capacity', '.', 'latest', 'from', 'our', 'local', 'hospital', 'is', 'they', 'will', 'only', 'test', 'those', 'with', 'symptoms', 'who', 'returned', 'from', 'china', ',', 'italy', ',', 'iran', ',', 'japan', ',', 'or', 'south', 'korea', 'in', 'last', '14', 'days', 'or', 'those', 'who', 'have', 'been', 'in', 'contact', 'with', 'someone', 'with', 'confirmed', 'co', '##vid', '-', '19', 'in', 'the', 'last', '14', 'days', '.']\n",
      "train Token IDs:  [2880, 2331, 9565, 2013, 1001, 2522, 17258, 16147, 1999, 1996, 2142, 2983, 2003, 2085, 3618, 2084, 1024, 2762, 1009, 3735, 1009, 5288, 1009, 5118, 1009, 5978, 1009, 5483, 1009, 4701, 1009, 6435, 1009, 5120, 1009, 3163, 1012, 1012, 1012, 4117, 1012, 2866, 1024, 6163, 1012, 1019, 2454, 1006, 22115, 2757, 1007, 2682, 2177, 1024, 15376, 2454, 1006, 11816, 2757, 1007, 8299, 3126, 2140]\n",
      "valid Token IDs:  [2005, 2216, 3038, 4501, 3475, 1521, 1056, 3304, 1025, 2044, 1017, 3134, 1997, 1996, 2034, 4484, 2553, 1010, 4501, 2038, 1037, 3020, 1001, 21887, 23350, 3930, 3446, 2084, 3304, 1012, 8519, 2006, 1996, 3277, 2360, 3304, 2001, 2205, 4030, 2000, 5843, 7698, 1012, 2130, 2044, 5843, 7698, 1010, 2009, 2165, 2051, 2005, 2009, 2000, 7919, 2031, 2019, 4254, 1012, 1001, 21887]\n",
      "test Token IDs:  [1030, 5310, 6643, 8323, 2123, 1521, 1056, 2031, 1996, 3977, 1012, 6745, 2013, 2256, 2334, 2902, 2003, 2027, 2097, 2069, 3231, 2216, 2007, 8030, 2040, 2513, 2013, 2859, 1010, 3304, 1010, 4238, 1010, 2900, 1010, 2030, 2148, 4420, 1999, 2197, 2403, 2420, 2030, 2216, 2040, 2031, 2042, 1999, 3967, 2007, 2619, 2007, 4484, 2522, 17258, 1011, 2539, 1999, 1996, 2197, 2403, 2420, 1012]\n"
     ]
    }
   ],
   "source": [
    "# Print the original sentence.\n",
    "print('train Original: ', train_sentences[0])\n",
    "print('valid Original: ', valid_sentences[0])\n",
    "print('test Original: ', test_sentences[0])\n",
    "\n",
    "# Print the sentence split into tokens.\n",
    "print('Tokenized train: ', tokenizer.tokenize(train_sentences[0]))\n",
    "print('Tokenized valid: ', tokenizer.tokenize(valid_sentences[0]))\n",
    "print('Tokenized test: ', tokenizer.tokenize(test_sentences[0]))\n",
    "\n",
    "# Print the sentence mapped to token ids.\n",
    "print('train Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(train_sentences[0])))\n",
    "print('valid Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(valid_sentences[0])))\n",
    "print('test Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(test_sentences[0])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "impossible-roberts",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1104 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train max sentence length:  1461\n",
      "valid max sentence length:  99\n",
      "test max sentence length:  114\n"
     ]
    }
   ],
   "source": [
    "train_max_len = 0\n",
    "valid_max_len = 0\n",
    "test_max_len = 0\n",
    "\n",
    "# Decide on a constant maximum sentence length for padding / truncating to \n",
    "# by choosing the max length of the sentences in the dataset.\n",
    "for sent in train_sentences:\n",
    "  # Tokenize the text and add `[CLS]` and `[SEP]` tokens.\n",
    "  input_ids = tokenizer.encode(sent, add_special_tokens=True)\n",
    "\n",
    "  # Update the maximum sentence length.\n",
    "  train_max_len = max(train_max_len, len(input_ids))\n",
    "\n",
    "for sent in valid_sentences:\n",
    "  # Tokenize the text and add `[CLS]` and `[SEP]` tokens.\n",
    "  input_ids = tokenizer.encode(sent, add_special_tokens=True)\n",
    "\n",
    "  # Update the maximum sentence length.\n",
    "  valid_max_len = max(valid_max_len, len(input_ids))\n",
    "\n",
    "for sent in test_sentences:\n",
    "  # Tokenize the text and add `[CLS]` and `[SEP]` tokens.\n",
    "  input_ids = tokenizer.encode(sent, add_special_tokens=True)\n",
    "\n",
    "  # Update the maximum sentence length.\n",
    "  test_max_len = max(test_max_len, len(input_ids))\n",
    "\n",
    "print('train max sentence length: ', train_max_len)\n",
    "print('valid max sentence length: ', valid_max_len)\n",
    "print('test max sentence length: ', test_max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "varied-little",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/boney/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2068: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Original sentence:  Official death toll from #covid19 in the United Kingdom is now GREATER than: Germany + Poland + Switzerland + Austria + Portugal + Greece + Sweden + Finland + Norway + Ireland... COMBINED. UK: 67.5 Million (233 dead) Above group: 185 Million (230 dead) HTTPURL\n",
      "train Token IDs list: tensor([  101,  2880,  2331,  9565,  2013,  1001,  2522, 17258, 16147,  1999,\n",
      "         1996,  2142,  2983,  2003,  2085,  3618,  2084,  1024,  2762,  1009,\n",
      "         3735,  1009,  5288,  1009,  5118,  1009,  5978,  1009,  5483,  1009,\n",
      "         4701,  1009,  6435,  1009,  5120,  1009,  3163,  1012,  1012,  1012,\n",
      "         4117,  1012,  2866,  1024,  6163,  1012,  1019,  2454,  1006, 22115,\n",
      "         2757,  1007,  2682,  2177,  1024, 15376,  2454,  1006, 11816,  2757,\n",
      "         1007,  8299,  3126,   102])\n",
      "validation Original sentence:  For those saying Pakistan isn‚Äôt Italy; After 3 weeks of the first confirmed case, Pakistan has a higher #Coronavirus growth rate than Italy. Experts on the issue say Italy was too slow to lockdown. Even after lockdown, it took time for it to properly have an impact. #Corona\n",
      "validation Token IDs list: tensor([  101,  2005,  2216,  3038,  4501,  3475,  1521,  1056,  3304,  1025,\n",
      "         2044,  1017,  3134,  1997,  1996,  2034,  4484,  2553,  1010,  4501,\n",
      "         2038,  1037,  3020,  1001, 21887, 23350,  3930,  3446,  2084,  3304,\n",
      "         1012,  8519,  2006,  1996,  3277,  2360,  3304,  2001,  2205,  4030,\n",
      "         2000,  5843,  7698,  1012,  2130,  2044,  5843,  7698,  1010,  2009,\n",
      "         2165,  2051,  2005,  2009,  2000,  7919,  2031,  2019,  4254,  1012,\n",
      "         1001, 21887,   102,     0])\n"
     ]
    }
   ],
   "source": [
    "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
    "train_input_ids = []\n",
    "train_attention_masks = []\n",
    "valid_input_ids = []\n",
    "valid_attention_masks = []\n",
    "\n",
    "# For every sentence...\n",
    "for sent in train_sentences:\n",
    "    # `encode_plus` will:\n",
    "    #   (1) Tokenize the sentence.\n",
    "    #   (2) Prepend the `[CLS]` token to the start.\n",
    "    #   (3) Append the `[SEP]` token to the end.\n",
    "    #   (4) Map tokens to their IDs.\n",
    "    #   (5) Pad or truncate the sentence to `max_length`\n",
    "    #   (6) Create attention masks for [PAD] tokens.\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "                        sent,                      # Sentence to encode.\n",
    "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                        max_length = 64,           # Pad & truncate all sentences.\n",
    "                        pad_to_max_length = True,\n",
    "                        return_attention_mask = True,   # Construct attn. masks.\n",
    "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                   )\n",
    "    \n",
    "    # Add the encoded sentence to the list.    \n",
    "    train_input_ids.append(encoded_dict['input_ids'])\n",
    "    \n",
    "    # And its attention mask (simply differentiates padding from non-padding).\n",
    "    train_attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "\n",
    "# For every sentence...\n",
    "for sent in valid_sentences:\n",
    "    # `encode_plus` will:\n",
    "    #   (1) Tokenize the sentence.\n",
    "    #   (2) Prepend the `[CLS]` token to the start.\n",
    "    #   (3) Append the `[SEP]` token to the end.\n",
    "    #   (4) Map tokens to their IDs.\n",
    "    #   (5) Pad or truncate the sentence to `max_length`\n",
    "    #   (6) Create attention masks for [PAD] tokens.\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "                        sent,                      # Sentence to encode.\n",
    "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                        max_length = 64,           # Pad & truncate all sentences.\n",
    "                        pad_to_max_length = True,\n",
    "                        return_attention_mask = True,   # Construct attn. masks.\n",
    "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                   )\n",
    "    \n",
    "    # Add the encoded sentence to the list.    \n",
    "    valid_input_ids.append(encoded_dict['input_ids'])\n",
    "    \n",
    "    # And its attention mask (simply differentiates padding from non-padding).\n",
    "    valid_attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "\n",
    "# Convert the lists into tensors.\n",
    "train_input_ids = torch.cat(train_input_ids, dim=0)\n",
    "train_attention_masks = torch.cat(train_attention_masks, dim=0)\n",
    "\n",
    "valid_input_ids = torch.cat(valid_input_ids, dim=0)\n",
    "valid_attention_masks = torch.cat(valid_attention_masks, dim=0)\n",
    "\n",
    "train_labels = torch.tensor(train_labels)\n",
    "valid_labels = torch.tensor(valid_labels)\n",
    "\n",
    "# Print sentence 0, now as a list of IDs.\n",
    "print('train Original sentence: ', train_sentences[0])\n",
    "print('train Token IDs list:', train_input_ids[0])\n",
    "\n",
    "print('validation Original sentence: ', valid_sentences[0])\n",
    "print('validation Token IDs list:', valid_input_ids[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "annual-romania",
   "metadata": {},
   "source": [
    "# Electra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "gorgeous-relative",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import torch\n",
    "from transformers import ElectraTokenizer, ElectraForSequenceClassification,AdamW\n",
    "import torch\n",
    "from sklearn.metrics import classification_report\n",
    "import random\n",
    "import time\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "distant-dress",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We will use the GPU: GeForce RTX 2080 Ti\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():  \n",
    "    device = torch.device(\"cuda\")\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))    \n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "heavy-cattle",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.rename(columns={'Label': 'target'})\n",
    "train = train.rename(columns={'Id': 'id'})\n",
    "train = train.rename(columns={'Text': 'text'})\n",
    "\n",
    "valid = valid.rename(columns={'Label': 'target'})\n",
    "valid = valid.rename(columns={'Id': 'id'})\n",
    "valid = valid.rename(columns={'Text': 'text'})\n",
    "\n",
    "test = test.rename(columns={'Label': 'target'})\n",
    "test = test.rename(columns={'Id': 'id'})\n",
    "test = test.rename(columns={'Text': 'text'})\n",
    "\n",
    "frames = [train, valid]\n",
    "df_train= pd.concat(frames)\n",
    "df_test= test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "returning-anchor",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    text=text.lower()\n",
    "    text = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', text)\n",
    "    text = re.sub(r'http?:\\/\\/.*[\\r\\n]*', '', text)\n",
    "    text=text.replace(r'&amp;?',r'and')\n",
    "    text=text.replace(r'&lt;',r'<')\n",
    "    text=text.replace(r'&gt;',r'>')\n",
    "    text = re.sub(r\"(?:\\@)\\w+\", '', text)\n",
    "    text=text.encode(\"ascii\",errors=\"ignore\").decode()\n",
    "    text=re.sub(r'[:\"#$%&\\*+,-/:;<=>@\\\\^_`{|}~]+','',text)\n",
    "    text=re.sub(r'[!]+','!',text)\n",
    "    text=re.sub(r'[?]+','?',text)\n",
    "    text=re.sub(r'[.]+','.',text)\n",
    "    text=re.sub(r\"'\",\"\",text)\n",
    "    text=re.sub(r\"\\(\",\"\",text)\n",
    "    text=re.sub(r\"\\)\",\"\",text)\n",
    "    text=\" \".join(text.split())\n",
    "    return text\n",
    "df_train['text'] = df_train['text'].apply(preprocess)\n",
    "df_test['text'] = df_test['text'].apply(preprocess)\n",
    "df_train=df_train[df_train[\"text\"]!='']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "sharing-jones",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train=df_train[[\"text\",\"target\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "given-copying",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = df_train.text.values\n",
    "labels = df_train.target.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "foreign-punch",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/electra-base-discriminator were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense_prediction.bias']\n",
      "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-base-discriminator and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ElectraForSequenceClassification(\n",
       "  (electra): ElectraModel(\n",
       "    (embeddings): ElectraEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): ElectraEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): ElectraLayer(\n",
       "          (attention): ElectraAttention(\n",
       "            (self): ElectraSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): ElectraSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ElectraIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): ElectraOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): ElectraLayer(\n",
       "          (attention): ElectraAttention(\n",
       "            (self): ElectraSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): ElectraSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ElectraIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): ElectraOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): ElectraLayer(\n",
       "          (attention): ElectraAttention(\n",
       "            (self): ElectraSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): ElectraSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ElectraIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): ElectraOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): ElectraLayer(\n",
       "          (attention): ElectraAttention(\n",
       "            (self): ElectraSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): ElectraSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ElectraIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): ElectraOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): ElectraLayer(\n",
       "          (attention): ElectraAttention(\n",
       "            (self): ElectraSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): ElectraSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ElectraIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): ElectraOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): ElectraLayer(\n",
       "          (attention): ElectraAttention(\n",
       "            (self): ElectraSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): ElectraSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ElectraIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): ElectraOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): ElectraLayer(\n",
       "          (attention): ElectraAttention(\n",
       "            (self): ElectraSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): ElectraSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ElectraIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): ElectraOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): ElectraLayer(\n",
       "          (attention): ElectraAttention(\n",
       "            (self): ElectraSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): ElectraSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ElectraIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): ElectraOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): ElectraLayer(\n",
       "          (attention): ElectraAttention(\n",
       "            (self): ElectraSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): ElectraSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ElectraIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): ElectraOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): ElectraLayer(\n",
       "          (attention): ElectraAttention(\n",
       "            (self): ElectraSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): ElectraSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ElectraIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): ElectraOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): ElectraLayer(\n",
       "          (attention): ElectraAttention(\n",
       "            (self): ElectraSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): ElectraSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ElectraIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): ElectraOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): ElectraLayer(\n",
       "          (attention): ElectraAttention(\n",
       "            (self): ElectraSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): ElectraSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ElectraIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): ElectraOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): ElectraClassificationHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=768, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "tokenizer = ElectraTokenizer.from_pretrained('google/electra-base-discriminator')\n",
    "model = ElectraForSequenceClassification.from_pretrained('google/electra-base-discriminator',num_labels=2)\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "polyphonic-copyright",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices=tokenizer.batch_encode_plus(texts,max_length=64,add_special_tokens=True, return_attention_mask=True,padding='max_length',truncation=True)\n",
    "input_ids=indices[\"input_ids\"]\n",
    "attention_masks=indices[\"attention_mask\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "sharp-junior",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, labels, \n",
    "                                                            random_state=42, test_size=0.2)\n",
    "\n",
    "train_masks, validation_masks, _, _ = train_test_split(attention_masks, labels,\n",
    "                                             random_state=42, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "noticed-national",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting all of our data into torch tensors\n",
    "train_inputs = torch.tensor(train_inputs)\n",
    "validation_inputs = torch.tensor(validation_inputs)\n",
    "train_labels = torch.tensor(train_labels, dtype=torch.long)\n",
    "validation_labels = torch.tensor(validation_labels, dtype=torch.long)\n",
    "train_masks = torch.tensor(train_masks, dtype=torch.long)\n",
    "validation_masks = torch.tensor(validation_masks, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "southwest-allah",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "# the DataLoader for our training set.\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "# the DataLoader for our validation set.\n",
    "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
    "validation_sampler = SequentialSampler(validation_data)\n",
    "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "unlikely-stockholm",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr = 6e-6, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
    "                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
    "                )\n",
    "\n",
    "\n",
    "# Number of training epochs (authors recommend between 2 and 4)\n",
    "epochs = 5\n",
    "\n",
    "# Total number of training steps is number of batches * number of epochs.\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "# Create the learning rate scheduler.\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps = 0, \n",
    "                                            num_training_steps = total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "defined-dietary",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate the accuracy of our predictions vs labels\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "accessible-consensus",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_time(elapsed):\n",
    "    '''\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    '''\n",
    "    # Round to the nearest second.\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    \n",
    "    # Format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "shared-scott",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 5 ========\n",
      "Training...\n",
      "  Batch    50  of    199.    Elapsed: 0:00:07.\n",
      "  Batch   100  of    199.    Elapsed: 0:00:14.\n",
      "  Batch   150  of    199.    Elapsed: 0:00:21.\n",
      "  Average training loss: 0.48\n",
      "  Training epoch took: 0:00:28\n",
      "\n",
      "======== Epoch 2 / 5 ========\n",
      "Training...\n",
      "  Batch    50  of    199.    Elapsed: 0:00:07.\n",
      "  Batch   100  of    199.    Elapsed: 0:00:14.\n",
      "  Batch   150  of    199.    Elapsed: 0:00:22.\n",
      "  Average training loss: 0.24\n",
      "  Training epoch took: 0:00:29\n",
      "\n",
      "======== Epoch 3 / 5 ========\n",
      "Training...\n",
      "  Batch    50  of    199.    Elapsed: 0:00:07.\n",
      "  Batch   100  of    199.    Elapsed: 0:00:15.\n",
      "  Batch   150  of    199.    Elapsed: 0:00:22.\n",
      "  Average training loss: 0.17\n",
      "  Training epoch took: 0:00:30\n",
      "\n",
      "======== Epoch 4 / 5 ========\n",
      "Training...\n",
      "  Batch    50  of    199.    Elapsed: 0:00:08.\n",
      "  Batch   100  of    199.    Elapsed: 0:00:15.\n",
      "  Batch   150  of    199.    Elapsed: 0:00:23.\n",
      "  Average training loss: 0.14\n",
      "  Training epoch took: 0:00:31\n",
      "\n",
      "======== Epoch 5 / 5 ========\n",
      "Training...\n",
      "  Batch    50  of    199.    Elapsed: 0:00:08.\n",
      "  Batch   100  of    199.    Elapsed: 0:00:16.\n",
      "  Batch   150  of    199.    Elapsed: 0:00:23.\n",
      "  Average training loss: 0.12\n",
      "  Training epoch took: 0:00:31\n",
      "\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "# Set the seed value all over the place to make this reproducible.\n",
    "seed_val = 42\n",
    "\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "# Store the average loss after each epoch so we can plot them.\n",
    "loss_values = []\n",
    "\n",
    "# For each epoch...\n",
    "for epoch_i in range(0, epochs):\n",
    "    \n",
    "    # Perform one full pass over the training set.\n",
    "    print(\"\")\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "    print('Training...')\n",
    "\n",
    "    # Measure how long the training epoch takes.\n",
    "    t0 = time.time()\n",
    "\n",
    "    # Reset the total loss for this epoch.\n",
    "    total_loss = 0\n",
    "\n",
    "    # Put the model into training mode. Don't be mislead--the call to \n",
    "    model.train()\n",
    "\n",
    "    # For each batch of training data...\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "\n",
    "        # Progress update every 100 batches.\n",
    "        if step % 50 == 0 and not step == 0:\n",
    "            # Calculate elapsed time in minutes.\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "            \n",
    "            # Report progress.\n",
    "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
    "\n",
    "        # As we unpack the batch, we'll also copy each tensor to the GPU  \n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "\n",
    "        model.zero_grad()        \n",
    "\n",
    "        outputs = model(b_input_ids, \n",
    "                    token_type_ids=None, \n",
    "                    attention_mask=b_input_mask, \n",
    "                    labels=b_labels)\n",
    "\n",
    "        loss = outputs[0]\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "    # Calculate the average loss over the training data.\n",
    "    avg_train_loss = total_loss / len(train_dataloader)            \n",
    "    \n",
    "    # Store the loss value for plotting the learning curve.\n",
    "    loss_values.append(avg_train_loss)\n",
    "\n",
    "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "    print(\"  Training epoch took: {:}\".format(format_time(time.time() - t0)))\n",
    "      \n",
    "print(\"\")\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "signed-kernel",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running Validation...\n",
      "Accuracy: 0.93\n",
      "Validation took: 0:00:02\n"
     ]
    }
   ],
   "source": [
    "# Validation               \n",
    "\n",
    "print(\"\")\n",
    "print(\"Running Validation...\")\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "model.eval()\n",
    "\n",
    "preds=[]\n",
    "true=[]\n",
    "\n",
    "eval_loss, eval_accuracy = 0, 0\n",
    "nb_eval_steps, nb_eval_examples = 0, 0\n",
    "\n",
    "# Evaluate data for one epoch\n",
    "for batch in validation_dataloader:\n",
    "    \n",
    "    # Add batch to GPU\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "    \n",
    "    # Unpack the inputs from our dataloader\n",
    "    b_input_ids, b_input_mask, b_labels = batch\n",
    "    \n",
    "    # Telling the model not to compute or store gradients, saving memory and\n",
    "    # speeding up validation\n",
    "    with torch.no_grad():        \n",
    "\n",
    "        # Forward pass, calculate logit predictions.\n",
    "        # This will return the logits rather than the loss because we have\n",
    "        # not provided labels.\n",
    "        # token_type_ids is the same as the \"segment ids\", which \n",
    "        # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
    "\n",
    "        outputs = model(b_input_ids, \n",
    "                        token_type_ids=None, \n",
    "                        attention_mask=b_input_mask)\n",
    "    \n",
    "    # Get the \"logits\" output by the model. The \"logits\" are the output\n",
    "    # values prior to applying an activation function like the softmax.\n",
    "    logits = outputs[0]\n",
    "\n",
    "    # Move logits and labels to CPU\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    label_ids = b_labels.to('cpu').numpy()\n",
    "    \n",
    "    preds.append(logits)\n",
    "    true.append(label_ids)\n",
    "    # Calculate the accuracy for this batch of test sentences.\n",
    "    tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
    "    \n",
    "    # Accumulate the total accuracy.\n",
    "    eval_accuracy += tmp_eval_accuracy\n",
    "\n",
    "    # Track the number of batches\n",
    "    nb_eval_steps += 1\n",
    "\n",
    "# Report the final accuracy for this validation run.\n",
    "print(\"Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n",
    "print(\"Validation took: {:}\".format(format_time(time.time() - t0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alive-minneapolis",
   "metadata": {},
   "source": [
    "# Electra - Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "indonesian-start",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of test sentences: 2,000\n",
      "\n",
      "test Original sentence:  @USER PA hospitals don‚Äôt have the capacity. Latest from our local hospital is they will only test those with symptoms who returned from China, Italy, Iran, Japan , or South Korea in last 14 days or those who have been in contact with someone with confirmed COVID-19 in the last 14 days.\n",
      "test Token IDs list: tensor([  101,  1030,  5310,  6643,  8323,  2123,  1521,  1056,  2031,  1996,\n",
      "         3977,  1012,  6745,  2013,  2256,  2334,  2902,  2003,  2027,  2097,\n",
      "         2069,  3231,  2216,  2007,  8030,  2040,  2513,  2013,  2859,  1010,\n",
      "         3304,  1010,  4238,  1010,  2900,  1010,  2030,  2148,  4420,  1999,\n",
      "         2197,  2403,  2420,  2030,  2216,  2040,  2031,  2042,  1999,  3967,\n",
      "         2007,  2619,  2007,  4484,  2522, 17258,  1011,  2539,  1999,  1996,\n",
      "         2197,  2403,  2420,   102])\n"
     ]
    }
   ],
   "source": [
    "# Report the number of sentences.\n",
    "print('Number of test sentences: {:,}\\n'.format(test.shape[0]))\n",
    "\n",
    "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
    "test_input_ids = []\n",
    "test_attention_masks = []\n",
    "\n",
    "# For every sentence...\n",
    "for sent in test_sentences:\n",
    "    # `encode_plus` will:\n",
    "    #   (1) Tokenize the sentence.\n",
    "    #   (2) Prepend the `[CLS]` token to the start.\n",
    "    #   (3) Append the `[SEP]` token to the end.\n",
    "    #   (4) Map tokens to their IDs.\n",
    "    #   (5) Pad or truncate the sentence to `max_length`\n",
    "    #   (6) Create attention masks for [PAD] tokens.\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "                        sent,                      # Sentence to encode.\n",
    "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                        max_length = 64,           # Pad & truncate all sentences.\n",
    "                        pad_to_max_length = True,\n",
    "                        return_attention_mask = True,   # Construct attn. masks.\n",
    "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                   )\n",
    "    \n",
    "    # Add the encoded sentence to the list.    \n",
    "    test_input_ids.append(encoded_dict['input_ids'])\n",
    "    \n",
    "    # And its attention mask (simply differentiates padding from non-padding).\n",
    "    test_attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "# Convert the lists into tensors.\n",
    "test_input_ids = torch.cat(test_input_ids, dim=0)\n",
    "test_attention_masks = torch.cat(test_attention_masks, dim=0)\n",
    "\n",
    "test_labels = torch.tensor(test_labels)\n",
    "\n",
    "# Print sentence 0, now as a list of IDs.\n",
    "print('test Original sentence: ', test_sentences[0])\n",
    "print('test Token IDs list:', test_input_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "invalid-harvey",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set test_data_set, the batch size and create DataLoader\n",
    "test_dataset = TensorDataset(test_input_ids, test_attention_masks, test_labels)\n",
    "\n",
    "batch_size = 32 # same as in training: 32  \n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "            test_dataset, # The test samples.\n",
    "            sampler = SequentialSampler(test_dataset), # Pull out batches sequentially.\n",
    "            batch_size = batch_size # Evaluate with this batch size.\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "juvenile-lunch",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting labels for 2,000 test sentences...\n",
      "  Accuracy: 0.87\n",
      "  Test Loss: 0.38\n",
      "  Test took  0:00:03 time\n"
     ]
    }
   ],
   "source": [
    "print(\"\")\n",
    "print('Predicting labels for {:,} test sentences...'.format(len(test_input_ids)))\n",
    "\n",
    "test_stats = []\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "# Put the model in evaluation mode--the dropout layers behave differently\n",
    "# during evaluation.\n",
    "model.eval()\n",
    "\n",
    "# Tracking variables \n",
    "total_eval_accuracy = 0\n",
    "total_eval_loss = 0\n",
    "nb_eval_steps = 0\n",
    "\n",
    "# Evaluate data for one epoch\n",
    "for batch in test_dataloader:\n",
    "\n",
    "    # Unpack this training batch from our dataloader. \n",
    "    # As we unpack the batch, I'll also copy each tensor to the GPU using the `to` method.\n",
    "    #\n",
    "    # `batch` contains three pytorch tensors:\n",
    "    #   [0]: input ids \n",
    "    #   [1]: attention masks\n",
    "    #   [2]: labels \n",
    "    b_input_ids = batch[0].to(device)\n",
    "    b_input_mask = batch[1].to(device)\n",
    "    b_labels = batch[2].to(device)\n",
    "\n",
    "    # Tell pytorch not to bother with constructing the compute graph during\n",
    "    # the forward pass, since this is only needed for backprop (training).\n",
    "    with torch.no_grad():        \n",
    "\n",
    "        # Forward pass, calculate logit predictions.\n",
    "        # token_type_ids is the same as the \"segment ids\", which \n",
    "        # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
    "        # Get the \"logits\" output by the model. The \"logits\" are the output\n",
    "        # values prior to applying an activation function like the softmax.\n",
    "        (loss, logits) = model(b_input_ids, \n",
    "                               token_type_ids=None, \n",
    "                               attention_mask=b_input_mask,\n",
    "                               labels=b_labels,\n",
    "                               return_dict=False)\n",
    "\n",
    "    # Accumulate the validation loss.\n",
    "    total_eval_loss += loss.item()\n",
    "\n",
    "    # Move logits and labels to CPU\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "    # Calculate the accuracy for this batch of test sentences, and\n",
    "    # accumulate it over all batches.\n",
    "    total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
    "\n",
    "\n",
    "# Report the final accuracy for this validation run.\n",
    "avg_test_accuracy = total_eval_accuracy / len(test_dataloader)\n",
    "print(\"  Accuracy: {0:.2f}\".format(avg_test_accuracy))\n",
    "\n",
    "# Calculate the average loss over all of the batches.\n",
    "avg_test_loss = total_eval_loss / len(test_dataloader)\n",
    "\n",
    "# Measure how long the validation run took.\n",
    "test_time = format_time(time.time() - t0)\n",
    "\n",
    "print(\"  Test Loss: {0:.2f}\".format(avg_test_loss))\n",
    "print(\"  Test took  {:} time\".format(test_time))\n",
    "\n",
    "# Record all statistics from this epoch.\n",
    "#    test_stats.append(\n",
    "#        {\n",
    "#            'Test. Loss': avg_val_loss,\n",
    "#            'Test. Accur.': avg_val_accuracy,\n",
    "#            'Test time': test_time\n",
    "#        }\n",
    "#    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
