{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tSDDdxquqpqz",
    "outputId": "fada05c7-2f15-4b39-de0f-c69245e561e6"
   },
   "outputs": [],
   "source": [
    "#!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "R3X1_EJ3qqfH"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, random_split\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import BertTokenizer, get_linear_schedule_with_warmup\n",
    "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "jRfe5op7qsyO"
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import time\n",
    "import datetime\n",
    "import random, re\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DhKpMFWnqvzl",
    "outputId": "4a2783be-0f55-4eb5-aea9-1057b5e73427"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are %d GPU(s) available. 2\n",
      "We will use the GPU: GeForce RTX 2080 Ti\n"
     ]
    }
   ],
   "source": [
    "# Check device \n",
    "# Get the GPU device name if available.\n",
    "if torch.cuda.is_available():    \n",
    "\n",
    "    # Tell PyTorch to use the GPU.    \n",
    "    device = torch.device(\"cuda\")\n",
    "\n",
    "    print('There are %d GPU(s) available. {}'.format(torch.cuda.device_count()))\n",
    "\n",
    "    print('We will use the GPU: {}'.format(torch.cuda.get_device_name(0)))\n",
    "\n",
    "# If we dont have GPU but a CPU, training will take place on CPU instead\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")\n",
    "    \n",
    "# Set the seed value all over the place to make this reproducible.\n",
    "seed_val = 42\n",
    "\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 797
    },
    "id": "KKwDdaNTq0Uq",
    "outputId": "3acd44b2-6055-434e-996c-da7716d18778"
   },
   "outputs": [],
   "source": [
    "# Read the data\n",
    "train = pd.read_csv('./nlp_covid/data/covid_train.tsv',sep='\\t')\n",
    "train = train.rename(columns={'Label': 'pre-Label'})\n",
    "\n",
    "valid = pd.read_csv('./nlp_covid/data/covid_valid.tsv', sep='\\t', names=['Id', 'Text', 'pre-Label'])\n",
    "\n",
    "test = pd.read_csv('./nlp_covid/data/covid_test.tsv',sep='\\t', names=['Id', 'Text', 'pre-Label'])\n",
    "\n",
    "train['Label'] = train['pre-Label'].apply(lambda x: 1 if x == 'INFORMATIVE' else 0)\n",
    "valid['Label'] = valid['pre-Label'].apply(lambda x: 1 if x == 'INFORMATIVE' else 0)\n",
    "test['Label'] = test['pre-Label'].apply(lambda x: 1 if x == 'INFORMATIVE' else 0)\n",
    "\n",
    "train = train.drop('pre-Label', axis = 1)\n",
    "test = test.drop('pre-Label', axis = 1)\n",
    "valid = valid.drop('pre-Label', axis = 1)\n",
    "\n",
    "pd.set_option('display.max_colwidth', 150)\n",
    "\n",
    "\n",
    "#print(train.info())\n",
    "#print(valid.info())\n",
    "#print(test.info())\n",
    "#print()\n",
    "#train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print(train.info())\n",
    "print(valid.info())\n",
    "print(test.info())\n",
    "print()\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "xzB8meOsq4Sg",
    "outputId": "57c096a7-06cd-4b48-bf0b-dbca540233ed"
   },
   "source": [
    "valid.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "PAqX3-0oq9cD",
    "outputId": "29dafe11-975c-4edf-fba6-502c3a8655a2"
   },
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GvE7vQEtq_0W",
    "outputId": "f791f801-4ee9-49a1-90b0-51ec5192913b"
   },
   "source": [
    "# Report the number of sentences.\n",
    "print('Number of training sentences: {:,}\\n'.format(train.shape[0]))\n",
    "print('Number of validation sentences: {:,}\\n'.format(valid.shape[0]))\n",
    "print('Number of test sentences: {:,}\\n'.format(test.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "nk2dyFKDrCce"
   },
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "def remove_url(text):\n",
    "  url = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "  return url.sub(r'', text)\n",
    "\n",
    "# df[\"text\"] = df[\"text\"].apply(lambda x: remove_url(x))\n",
    "    \n",
    "def remove_emoji(text):\n",
    "  emoji_pattern = re.compile(\"[\"\n",
    "                               u\"\\U0001F600-\\U0001F64F\" #emoticons\n",
    "                               u\"\\U0001F300-\\U0001F5FF\" #symbols&pics\n",
    "                               u\"\\U0001F680-\\U0001F6FF\" #transportation pic\n",
    "                               u\"\\U0001F1E0-\\U0001F1FF\" #flags\n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U000024C2-\\U0001F251\"    \n",
    "                               \"]+\", flags = re.UNICODE)\n",
    "  return emoji_pattern.sub(r'', text)\n",
    "\n",
    "# df[\"text\"] = df[\"text\"].apply(lambda x: remove_emoji(x))\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    return text.translate(table)\n",
    "\n",
    "# df[\"text\"] = df[\"text\"].apply(lambda x: remove_punctuation(x))\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    \n",
    "    text = text.lower() #lowercase\n",
    "    \n",
    "    text = re.sub(r'[!]+','!',text)\n",
    "    text = re.sub(r'[?]+','?',text)\n",
    "    #text = re.sub(r'[.]+','.',text)\n",
    "    #text = re.sub(r\"'\",\"\",text)\n",
    "    #text = re.sub('\\s+', '', text).strip() # Remove and double spaces\n",
    "    #text = re.sub(r'&amp;?',r'and', text) # replace & -> and\n",
    "    #text = re.sub(r\"https?:\\/\\/t.co\\/[A-Za-z0-9]+\", \"\", text) # Remove URLs\n",
    "    remove_url(text)\n",
    "    remove_emoji(text)\n",
    "    #text = re.sub(r'[\"$%\\*\\+,-\\/:;<=>@\\\\^_`{|}~]+','',text) #remove some puncts (except . ! # ? &)\n",
    "    \n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "2kRl_Cfur-UF",
    "outputId": "11ec96ba-f1e5-46ed-962b-d4101e600c28"
   },
   "outputs": [],
   "source": [
    "train['Text'] = train['Text'].apply(lambda x: clean_text(x))\n",
    "valid['Text'] = valid['Text'].apply(lambda x: clean_text(x))\n",
    "test['Text'] = test['Text'].apply(lambda x: clean_text(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "RHLwyj1dsBVd",
    "outputId": "938e28a5-7223-4be1-f5f6-00a06d83996f"
   },
   "source": [
    "train.head()\n",
    "valid.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "9_1-dcqrsDD_",
    "outputId": "081ed2b9-7bff-4fa4-9922-9b08fe547285"
   },
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "uxU9bf-ZrfLT",
    "outputId": "2edb1600-7da4-4d8c-fa8f-258f711e0382"
   },
   "source": [
    "# Plot count of informative and uninformative tweets in the train set\n",
    "\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.subplot(1, 2, 1)\n",
    "terms = ['Uninformative', 'Informative']\n",
    "sns.set_theme(style=\"darkgrid\")\n",
    "ax = sns.countplot(x=\"Label\", data=train, label = terms)\n",
    "ax.set_xticklabels(terms)\n",
    "weightage = [len(train[train['Label'] == 0]),len(train[train['Label'] == 1])]\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.pie(weightage,labels=terms, autopct=\"%1.1f%%\")\n",
    "plt.suptitle(\"Train Dataset label distribution\")\n",
    "plt.show()\n",
    "\n",
    "print('Number of uninformative (0) and informative (1) tweets contained in the train dataset:')\n",
    "print(train['Label'].value_counts())\n",
    "\n",
    "\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.subplot(1, 2, 1)\n",
    "terms = ['Uninformative', 'Informative']\n",
    "sns.set_theme(style=\"darkgrid\")\n",
    "ax = sns.countplot(x=\"Label\", data=valid, label = terms)\n",
    "ax.set_xticklabels(terms)\n",
    "weightage = [len(valid[valid['Label'] == 0]),len(valid[valid['Label'] == 1])]\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.pie(weightage,labels=terms, autopct=\"%1.1f%%\")\n",
    "plt.suptitle(\"Validation Dataset label distribution\")\n",
    "plt.show()\n",
    "\n",
    "print('Number of uninformative (0) and informative (1) tweets contained in the validation dataset:')\n",
    "print(valid['Label'].value_counts())\n",
    "\n",
    "\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.subplot(1, 2, 1)\n",
    "terms = ['Uninformative', 'Informative']\n",
    "sns.set_theme(style=\"darkgrid\")\n",
    "ax = sns.countplot(x=\"Label\", data=test, label = terms)\n",
    "ax.set_xticklabels(terms)\n",
    "weightage = [len(test[test['Label'] == 0]),len(test[test['Label'] == 1])]\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.pie(weightage,labels=terms, autopct=\"%1.1f%%\")\n",
    "plt.suptitle(\"Test Dataset label distribution\")\n",
    "plt.show()\n",
    "\n",
    "print('Number of uninformative (0) and informative (1) tweets contained in the test dataset:')\n",
    "print(test['Label'].value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import pickle\n",
    "\n",
    "train = pickle.load( open( \"covid_train.p\", \"rb\" ) )\n",
    "valid = pickle.load( open( \"covid_valid.p\", \"rb\" ) )\n",
    "test = pickle.load( open( \"covid_test.p\", \"rb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 165,
     "referenced_widgets": [
      "854738ab2edd485b97a2b0be4c4e3215",
      "cfdbb6ec3b004d1c9733c25d52c777ba",
      "4b4d0b221e3a49dd8dad21974d5fa44a",
      "a3076b34cf04458f8012359ce912d8a8",
      "4cf1975862094034b527fcd5c8d67be2",
      "38c3dbee3b8940379d8f48e0f88dfd59",
      "949bfeae43a847d3a0de2a40f3ed5579",
      "24f77f432f7b4e14a7947cd03c798dd9",
      "0a08d24cdddb4f248d62bf08499b7e2d",
      "a911c1406be3402997126058e34a39dd",
      "46a47186f0074ca9949b98927e61f4f5",
      "026ac54a29574a7f87bc31153572934d",
      "750e8382dd814da39e1294a57c4d1e7a",
      "dd52bd4a2d184559a0f4e50eb81107b6",
      "2e6cb02d5ed44204b83a639cbc75ada1",
      "6f3874a4359d4cd08a174b4e9d71c360",
      "f9751896cf88402dbad23aa403a94a6c",
      "3c5e113589ab4f949707a1539b1f9af0",
      "4eea6d5c9d794e6dbd1489b97bce5a42",
      "2f308c8f2b4c452f899d0f29a59b6b49",
      "8e4ae2ff85554d7d95db3f3571578f2c",
      "06facbbcca6b47e5b50a92aff13ef54c",
      "a0a46ac119ee4b439e8c6662122faad5",
      "154c6f4caee349919a49d8d4a2ddf266"
     ]
    },
    "id": "FoWJWAHxrnxM",
    "outputId": "aec3d944-4751-4592-8b95-cc0d1a1fcd1f"
   },
   "outputs": [],
   "source": [
    "# Get the lists of sentences and their labels.\n",
    "train_sentences_initial = train.Text.values\n",
    "#train_labels_initial = train.Label.values\n",
    "\n",
    "valid_sentences_initial = valid.Text.values\n",
    "#valid_labels_initial = valid.Label.values\n",
    "\n",
    "test_sentences_initial = test.Text.values\n",
    "#test_labels_initial = test.Label.values\n",
    "\n",
    "# Load the BERT tokenizer.\n",
    "#To feed our text to BERT, it must be split into tokens, and then these tokens must be \n",
    "#mapped to their index in the tokenizer vocabulary.\n",
    "#The tokenization must be performed by the tokenizer included with BERT\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cqGOVC9SrriS",
    "outputId": "0ce21d28-96f7-4455-a226-79d3652acba3"
   },
   "source": [
    "# Print the original sentence.\n",
    "print('train Original: ', train_sentences_initial[0])\n",
    "print('valid Original: ', valid_sentences_initial[0])\n",
    "print('test Original: ', test_sentences_initial[0])\n",
    "\n",
    "# Print the sentence split into tokens.\n",
    "print('Tokenized train: ', tokenizer.tokenize(train_sentences_initial[0]))\n",
    "print('Tokenized valid: ', tokenizer.tokenize(valid_sentences_initial[0]))\n",
    "print('Tokenized test: ', tokenizer.tokenize(test_sentences_initial[0]))\n",
    "\n",
    "# Print the sentence mapped to token ids.\n",
    "print('train Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(train_sentences_initial[0])))\n",
    "print('valid Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(valid_sentences_initial[0])))\n",
    "print('test Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(test_sentences_initial[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V3A5Z8KMrusc",
    "outputId": "2d83b930-26b7-4140-82fc-1641c20ca736"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1104 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train dataset length:  6936\n",
      "validation dataset length:  1000\n",
      "test dataset length:  2000\n",
      "\n",
      "train max sentence length:  1461\n",
      "valid max sentence length:  99\n",
      "test max sentence length:  114\n"
     ]
    }
   ],
   "source": [
    "train_max_len = 0\n",
    "valid_max_len = 0\n",
    "test_max_len = 0\n",
    "\n",
    "train_length_ls = []\n",
    "valid_length_ls = []\n",
    "test_length_ls = []\n",
    "\n",
    "# Decide on a constant maximum sentence length for padding / truncating to \n",
    "# by choosing the max length of the sentences in the dataset.\n",
    "for sent in train_sentences_initial:\n",
    "  # Tokenize the text and add `[CLS]` and `[SEP]` tokens.\n",
    "  input_ids = tokenizer.encode(sent, add_special_tokens=True)\n",
    "  train_length_ls.append(len(input_ids))\n",
    "\n",
    "  # Update the maximum sentence length.\n",
    "  # train_max_len = max(train_max_len, len(input_ids))\n",
    "\n",
    "for sent in valid_sentences_initial:\n",
    "  # Tokenize the text and add `[CLS]` and `[SEP]` tokens.\n",
    "  input_ids = tokenizer.encode(sent, add_special_tokens=True)\n",
    "  valid_length_ls.append(len(input_ids))\n",
    "\n",
    "  # Update the maximum sentence length.\n",
    "  # valid_max_len = max(valid_max_len, len(input_ids))\n",
    "\n",
    "for sent in test_sentences_initial:\n",
    "  # Tokenize the text and add `[CLS]` and `[SEP]` tokens.\n",
    "  input_ids = tokenizer.encode(sent, add_special_tokens=True)\n",
    "  test_length_ls.append(len(input_ids))\n",
    "\n",
    "  # Update the maximum sentence length.\n",
    "  # test_max_len = max(test_max_len, len(input_ids))\n",
    "\n",
    "\n",
    "train_length_df = pd.DataFrame(train_length_ls, columns = ['length'])\n",
    "valid_length_df = pd.DataFrame(valid_length_ls, columns = ['length'])\n",
    "test_length_df = pd.DataFrame(test_length_ls, columns = ['length'])\n",
    "\n",
    "train_max_len = max(train_length_ls)\n",
    "valid_max_len = max(valid_length_ls)\n",
    "test_max_len = max(test_length_ls)\n",
    "\n",
    "print('train dataset length: ', len(train_length_ls))\n",
    "print('validation dataset length: ', len(valid_length_ls))\n",
    "print('test dataset length: ', len(test_length_ls))\n",
    "print()\n",
    "\n",
    "\n",
    "print('train max sentence length: ', train_max_len)\n",
    "print('valid max sentence length: ', valid_max_len)\n",
    "print('test max sentence length: ', test_max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x5IIDoJAr08M",
    "outputId": "c514eac6-1e8a-4cbb-b650-1777016972ad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "longest 5 train: [141, 645, 1104, 1208, 1461]\n",
      "longest 5 valid: [91, 91, 92, 96, 99]\n",
      "longest 5 test: [90, 91, 101, 107, 114]\n"
     ]
    }
   ],
   "source": [
    "train_length_ls.sort()\n",
    "valid_length_ls.sort()\n",
    "test_length_ls.sort()\n",
    "\n",
    "print('longest 5 train:', train_length_ls[len(train_length_ls) - 5:])\n",
    "print('longest 5 valid:', valid_length_ls[len(valid_length_ls) - 5:])\n",
    "print('longest 5 test:', test_length_ls[len(test_length_ls) - 5:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GotT8hCnr4pR",
    "outputId": "2a121a3f-4d2b-49ae-c1fe-4d5ba48fe749"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      length  Label\n",
      "0         65      1\n",
      "1         66      1\n",
      "2         68      1\n",
      "3         58      1\n",
      "4         48      0\n",
      "...      ...    ...\n",
      "6931      58      0\n",
      "6932      39      1\n",
      "6933      60      0\n",
      "6934      56      0\n",
      "6935      40      0\n",
      "\n",
      "[6936 rows x 2 columns]\n",
      "     length  Label\n",
      "0        63      0\n",
      "1        58      1\n",
      "2        66      0\n",
      "3        50      0\n",
      "4        60      0\n",
      "..      ...    ...\n",
      "995      32      0\n",
      "996      58      1\n",
      "997      33      0\n",
      "998      57      0\n",
      "999      56      1\n",
      "\n",
      "[1000 rows x 2 columns]\n",
      "      length  Label\n",
      "0         65      0\n",
      "1         24      1\n",
      "2         45      0\n",
      "3         70      0\n",
      "4         42      0\n",
      "...      ...    ...\n",
      "1995      56      0\n",
      "1996      39      0\n",
      "1997      56      1\n",
      "1998      70      1\n",
      "1999      41      1\n",
      "\n",
      "[2000 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "train_length_df['Label'] = train['Label']\n",
    "valid_length_df['Label'] = valid['Label']\n",
    "test_length_df['Label'] = test['Label']\n",
    "\n",
    "print(train_length_df)\n",
    "print(valid_length_df)\n",
    "print(test_length_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "DcVhwzHFr7xg",
    "outputId": "033e24ac-6ffe-4feb-e8a0-23f790771a37"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>length</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>65</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>66</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>68</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>58</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>48</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6931</th>\n",
       "      <td>58</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6932</th>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6933</th>\n",
       "      <td>60</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6934</th>\n",
       "      <td>56</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6935</th>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6932 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      length  Label\n",
       "0         65      1\n",
       "1         66      1\n",
       "2         68      1\n",
       "3         58      1\n",
       "4         48      0\n",
       "...      ...    ...\n",
       "6931      58      0\n",
       "6932      39      1\n",
       "6933      60      0\n",
       "6934      56      0\n",
       "6935      40      0\n",
       "\n",
       "[6932 rows x 2 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drop_indices = train_length_df[train_length_df['length'].isin(train_length_ls[len(train_length_ls) - 4:])].index\n",
    "\n",
    "train_length_df.drop(drop_indices, inplace = True)\n",
    "\n",
    "train_length_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "lJLMO1oir_Xu",
    "outputId": "620ca754-386b-47ab-8a1e-274f3f851c7a"
   },
   "source": [
    "# comparison of length of tweets between uninformative and informative ones, after removing 4 outlier from training dataset\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "train_box_plot = [go.Box(y=train_length_df[train_length_df['Label']==0]['length'],name='Uninformative'),\n",
    "        go.Box(y=train_length_df[train_length_df['Label']==1]['length'],name='Informative')]\n",
    "layout = go.Layout(title = 'Comparison of text length in training dataset Tweets')\n",
    "fig = go.Figure(data=train_box_plot, layout=layout)\n",
    "fig.show()\n",
    "\n",
    "valid_box_plot = [go.Box(y=valid_length_df[valid_length_df['Label']==0]['length'],name='Uninformative'),\n",
    "        go.Box(y=valid_length_df[valid_length_df['Label']==1]['length'],name='Informative')]\n",
    "layout = go.Layout(title = 'Comparison of text length in validation dataset Tweets')\n",
    "fig = go.Figure(data=valid_box_plot, layout=layout)\n",
    "fig.show()\n",
    "\n",
    "test_box_plot = [go.Box(y=test_length_df[test_length_df['Label']==0]['length'],name='Uninformative'),\n",
    "        go.Box(y=test_length_df[test_length_df['Label']==1]['length'],name='Informative')]\n",
    "layout = go.Layout(title = 'Comparison of text length in test dataset Tweets')\n",
    "fig = go.Figure(data=test_box_plot, layout=layout)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "id": "yNOf60PNsCZ_",
    "outputId": "285577c9-728e-42c4-d88b-ba5c14b2a6c7"
   },
   "source": [
    "# display plot of sentence lengths after removing 4 outliers form training data set \n",
    "\n",
    "from plotly import graph_objs as go\n",
    "\n",
    "train_text_length = train_length_df['length'].value_counts().sort_index()\n",
    "valid_text_length = valid_length_df['length'].value_counts().sort_index()\n",
    "test_text_length = test_length_df['length'].value_counts().sort_index()\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=train_text_length.index,\n",
    "    y=train_text_length.values,\n",
    "    name='train sent lengths',\n",
    "    fill='tozeroy',\n",
    "    marker_color=\"cyan\",\n",
    "))\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=valid_text_length.index,\n",
    "    y=valid_text_length.values,\n",
    "    name='validation sent lengths',\n",
    "    fill='tozeroy',\n",
    "    marker_color=\"darkblue\",\n",
    "))\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=test_text_length.index,\n",
    "    y=test_text_length.values,\n",
    "    name='test sent lengths',\n",
    "    fill='tozeroy',\n",
    "    marker_color=\"chocolate\",\n",
    "))\n",
    "fig.update_layout(\n",
    "    title='<span style=\"font-size:32px; font-family:Times New Roman\">Sentence Lengths</span>',\n",
    "    xaxis_title = \"Sentence length\",\n",
    "    yaxis_title = \"Frequency\"\n",
    ")\n",
    "\n",
    "fig.update_xaxes(range=[0, 150])\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 280
    },
    "id": "kH94bfOtsIye",
    "outputId": "7ee82acc-52ac-4366-dc6f-4452f1e546a4"
   },
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "\n",
    "#twitter_mask = np.array(Image.open('/content/drive/MyDrive/DL for NLP/Project/twitter_mask.jpg'))\n",
    "\n",
    "# wc = WordCloud(\n",
    "#    background_color='blue', \n",
    "#    max_words=200, \n",
    "#    mask=twitter_mask,\n",
    "#)\n",
    "\n",
    "wc = WordCloud(width=800, height=500, random_state=21, max_font_size=110) \n",
    "\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.title('training data set', \n",
    "          fontdict={'size': 14,  'verticalalignment': 'bottom'})\n",
    "wc.generate(' '.join(text for text in train['Text']))\n",
    "plt.imshow(wc)\n",
    "plt.axis(\"off\")\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.title('validation data set', \n",
    "          fontdict={'size': 14,  'verticalalignment': 'bottom'})\n",
    "wc.generate(' '.join(text for text in valid['Text']))\n",
    "plt.imshow(wc)\n",
    "plt.axis(\"off\")\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.title('test data set', \n",
    "          fontdict={'size': 14,  'verticalalignment': 'bottom'})\n",
    "wc.generate(' '.join(text for text in test['Text']))\n",
    "plt.imshow(wc)\n",
    "plt.axis(\"off\")\n",
    "plt.suptitle('Top words')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 415
    },
    "id": "SRGO-VGJsM63",
    "outputId": "98f3c0bc-0305-4a66-b797-6669284f5ea7"
   },
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "import cufflinks as cf\n",
    "cf.go_offline()\n",
    "cf.set_config_file(offline=False, world_readable=True)\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "\n",
    "train_unigrams = pd.DataFrame()\n",
    "train_unigrams = train[['Text', 'Label']]\n",
    "train_unigrams['Text_no_punc'] = train_unigrams['Text'].apply(lambda x: remove_punctuation(x))\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words.add('httpurl')\n",
    "\n",
    "tokens = pd.DataFrame()\n",
    "tokens['tokenized_minus_stop'] = train_unigrams['Text_no_punc'].apply(lambda x: [tokens.lower() for tokens in word_tokenize(x) if tokens.lower() not in stop_words])\n",
    "tokens['Label'] = train_unigrams['Label']\n",
    "token_list_ui = [j for i in tokens[tokens['Label'] == 0]['tokenized_minus_stop'] for j in i]\n",
    "token_list_i = [j for i in tokens[tokens['Label'] == 1]['tokenized_minus_stop'] for j in i]\n",
    "\n",
    "token_counts_ui = Counter(token_list_ui)\n",
    "top_token_counts_ui = token_counts_ui.most_common(10)\n",
    "top_token_counts_ui =sorted(top_token_counts_ui, key = lambda x: x[1], reverse=True)\n",
    "\n",
    "token_counts_i = Counter(token_list_i)\n",
    "top_token_counts_i = token_counts_i.most_common(10)\n",
    "top_token_counts_i =sorted(top_token_counts_i, key = lambda x: x[1], reverse=True)\n",
    "\n",
    "x_uninformative, y_uninformative = map(list, zip(*top_token_counts_ui))\n",
    "x_informative, y_informative = map(list, zip(*top_token_counts_i))\n",
    "\n",
    "\n",
    "fig_unigram, ax_unigram = plt.subplots(1,2, figsize=(10,4))\n",
    "sns.barplot(x=y_uninformative, y=x_uninformative, orient='h', palette=\"Reds_d\", ax=ax_unigram[0])\n",
    "sns.barplot(x=y_informative, y=x_informative, orient='h', palette=\"Blues_d\", ax=ax_unigram[1])\n",
    "\n",
    "\n",
    "ax_unigram[0].set_title(\"Top 10 unigrams - Uninformative Tweets\")\n",
    "ax_unigram[0].set_xlabel(\"Word Frequency\")\n",
    "ax_unigram[1].set_title(\"Top 10 unigrams - Informative Tweets\")\n",
    "ax_unigram[1].set_xlabel(\"Word Frequency\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "C3kbE34ucJYZ"
   },
   "outputs": [],
   "source": [
    "\n",
    "train.drop(drop_indices, inplace = True)\n",
    "\n",
    "train_sentences = train.Text.values\n",
    "train_labels = train.Label.values\n",
    "\n",
    "valid_sentences = valid.Text.values\n",
    "valid_labels = valid.Label.values\n",
    "\n",
    "test_sentences = test.Text.values\n",
    "test_labels = test.Label.values\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CfcmzTYGhHDx",
    "outputId": "5cd40fba-e9ac-4e38-afd7-a1de5e78c751"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train dataset length:  6932\n",
      "validation dataset length:  1000\n",
      "test dataset length:  2000\n",
      "\n",
      "train max sentence length:  141\n",
      "valid max sentence length:  99\n",
      "test max sentence length:  114\n"
     ]
    }
   ],
   "source": [
    "train_max_len_v2 = 0\n",
    "valid_max_len_v2 = 0\n",
    "test_max_len_v2 = 0\n",
    "\n",
    "train_length_ls_v2 = []\n",
    "valid_length_ls_v2 = []\n",
    "test_length_ls_v2 = []\n",
    "\n",
    "# Decide on a constant maximum sentence length for padding / truncating to \n",
    "# by choosing the max length of the sentences in the dataset.\n",
    "for sent in train_sentences:\n",
    "  # Tokenize the text and add `[CLS]` and `[SEP]` tokens.\n",
    "  input_ids = tokenizer.encode(sent, add_special_tokens=True)\n",
    "  train_length_ls_v2.append(len(input_ids))\n",
    "\n",
    "  # Update the maximum sentence length.\n",
    "  # train_max_len = max(train_max_len, len(input_ids))\n",
    "\n",
    "for sent in valid_sentences:\n",
    "  # Tokenize the text and add `[CLS]` and `[SEP]` tokens.\n",
    "  input_ids = tokenizer.encode(sent, add_special_tokens=True)\n",
    "  valid_length_ls_v2.append(len(input_ids))\n",
    "\n",
    "  # Update the maximum sentence length.\n",
    "  # valid_max_len = max(valid_max_len, len(input_ids))\n",
    "\n",
    "for sent in test_sentences:\n",
    "  # Tokenize the text and add `[CLS]` and `[SEP]` tokens.\n",
    "  input_ids = tokenizer.encode(sent, add_special_tokens=True)\n",
    "  test_length_ls_v2.append(len(input_ids))\n",
    "\n",
    "  # Update the maximum sentence length.\n",
    "  # test_max_len = max(test_max_len, len(input_ids))\n",
    "\n",
    "train_max_len_v2 = max(train_length_ls_v2)\n",
    "valid_max_len_v2 = max(valid_length_ls_v2)\n",
    "test_max_len_v2 = max(test_length_ls_v2)\n",
    "\n",
    "print('train dataset length: ', len(train_length_ls_v2))\n",
    "print('validation dataset length: ', len(valid_length_ls_v2))\n",
    "print('test dataset length: ', len(test_length_ls_v2))\n",
    "print()\n",
    "\n",
    "print('train max sentence length: ', train_max_len_v2)\n",
    "print('valid max sentence length: ', valid_max_len_v2)\n",
    "print('test max sentence length: ', test_max_len_v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-Dt2RqlRsTti",
    "outputId": "27e0c2b0-b1ba-40be-cf2a-4e94b33a2236"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/boney/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2068: FutureWarning:\n",
      "\n",
      "The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Original sentence:  official death toll from #covid19 in the united kingdom is now greater than: germany + poland + switzerland + austria + portugal + greece + sweden + finland + norway + ireland... combined. uk: 67.5 million (233 dead) above group: 185 million (230 dead) httpurl\n",
      "train Token IDs list: tensor([  101,  2880,  2331,  9565,  2013,  1001,  2522, 17258, 16147,  1999,\n",
      "         1996,  2142,  2983,  2003,  2085,  3618,  2084,  1024,  2762,  1009,\n",
      "         3735,  1009,  5288,  1009,  5118,  1009,  5978,  1009,  5483,  1009,\n",
      "         4701,  1009,  6435,  1009,  5120,  1009,  3163,  1012,  1012,  1012,\n",
      "         4117,  1012,  2866,  1024,  6163,  1012,  1019,  2454,  1006, 22115,\n",
      "         2757,  1007,  2682,  2177,  1024, 15376,  2454,  1006, 11816,  2757,\n",
      "         1007,  8299,  3126,   102])\n",
      "validation Original sentence:  for those saying pakistan isn’t italy; after 3 weeks of the first confirmed case, pakistan has a higher #coronavirus growth rate than italy. experts on the issue say italy was too slow to lockdown. even after lockdown, it took time for it to properly have an impact. #corona\n",
      "validation Token IDs list: tensor([  101,  2005,  2216,  3038,  4501,  3475,  1521,  1056,  3304,  1025,\n",
      "         2044,  1017,  3134,  1997,  1996,  2034,  4484,  2553,  1010,  4501,\n",
      "         2038,  1037,  3020,  1001, 21887, 23350,  3930,  3446,  2084,  3304,\n",
      "         1012,  8519,  2006,  1996,  3277,  2360,  3304,  2001,  2205,  4030,\n",
      "         2000,  5843,  7698,  1012,  2130,  2044,  5843,  7698,  1010,  2009,\n",
      "         2165,  2051,  2005,  2009,  2000,  7919,  2031,  2019,  4254,  1012,\n",
      "         1001, 21887,   102,     0])\n"
     ]
    }
   ],
   "source": [
    "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
    "train_input_ids = []\n",
    "train_attention_masks = []\n",
    "valid_input_ids = []\n",
    "valid_attention_masks = []\n",
    "\n",
    "# For every sentence...\n",
    "for sent in train_sentences:\n",
    "    # `encode_plus` will:\n",
    "    #   (1) Tokenize the sentence.\n",
    "    #   (2) Prepend the `[CLS]` token to the start.\n",
    "    #   (3) Append the `[SEP]` token to the end.\n",
    "    #   (4) Map tokens to their IDs.\n",
    "    #   (5) Pad or truncate the sentence to `max_length`\n",
    "    #   (6) Create attention masks for [PAD] tokens.\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "                        sent,                      # Sentence to encode.\n",
    "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                        max_length = 64,           # Pad & truncate all sentences.\n",
    "                        pad_to_max_length = True,\n",
    "                        return_attention_mask = True,   # Construct attn. masks.\n",
    "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                   )\n",
    "    \n",
    "    # Add the encoded sentence to the list.    \n",
    "    train_input_ids.append(encoded_dict['input_ids'])\n",
    "    \n",
    "    # And its attention mask (simply differentiates padding from non-padding).\n",
    "    train_attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "\n",
    "# For every sentence...\n",
    "for sent in valid_sentences:\n",
    "    # `encode_plus` will:\n",
    "    #   (1) Tokenize the sentence.\n",
    "    #   (2) Prepend the `[CLS]` token to the start.\n",
    "    #   (3) Append the `[SEP]` token to the end.\n",
    "    #   (4) Map tokens to their IDs.\n",
    "    #   (5) Pad or truncate the sentence to `max_length`\n",
    "    #   (6) Create attention masks for [PAD] tokens.\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "                        sent,                      # Sentence to encode.\n",
    "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                        max_length = 64,           # Pad & truncate all sentences.\n",
    "                        pad_to_max_length = True,\n",
    "                        return_attention_mask = True,   # Construct attn. masks.\n",
    "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                   )\n",
    "    \n",
    "    # Add the encoded sentence to the list.    \n",
    "    valid_input_ids.append(encoded_dict['input_ids'])\n",
    "    \n",
    "    # And its attention mask (simply differentiates padding from non-padding).\n",
    "    valid_attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "\n",
    "# Convert the lists into tensors.\n",
    "train_input_ids = torch.cat(train_input_ids, dim=0)\n",
    "train_attention_masks = torch.cat(train_attention_masks, dim=0)\n",
    "\n",
    "valid_input_ids = torch.cat(valid_input_ids, dim=0)\n",
    "valid_attention_masks = torch.cat(valid_attention_masks, dim=0)\n",
    "\n",
    "train_labels = torch.tensor(train_labels)\n",
    "valid_labels = torch.tensor(valid_labels)\n",
    "\n",
    "# Print sentence 0, now as a list of IDs.\n",
    "print('train Original sentence: ', train_sentences[0])\n",
    "print('train Token IDs list:', train_input_ids[0])\n",
    "\n",
    "print('validation Original sentence: ', valid_sentences[0])\n",
    "print('validation Token IDs list:', valid_input_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "7a_pjve5seoz"
   },
   "outputs": [],
   "source": [
    "# Combine the training inputs into a TensorDataset.\n",
    "train_dataset = TensorDataset(train_input_ids, train_attention_masks, train_labels)\n",
    "valid_dataset = TensorDataset(valid_input_ids, valid_attention_masks, valid_labels)\n",
    "\n",
    "# Create a 80-20 train-validation split.\n",
    "# Calculate the number of samples to include in each set.\n",
    "# train_size = int(0.8 * len(dataset))\n",
    "# val_size = len(dataset) - train_size\n",
    "\n",
    "# Divide the dataset by randomly selecting samples.\n",
    "# train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# print('{:>5,} training samples'.format(train_size))\n",
    "# print('{:>5,} validation samples'.format(val_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "coOcc25FsjNJ"
   },
   "outputs": [],
   "source": [
    "# Define a batch size value for the DataLoader\n",
    "# It is not recommended to use a high value, usually batch size of 16 or 32 works perfectly\n",
    "batch_size = 32\n",
    "\n",
    "# Create the DataLoaders for train set with the training samples in random order\n",
    "train_dataloader = DataLoader(\n",
    "            train_dataset,  # The training samples.\n",
    "            sampler = RandomSampler(train_dataset), # Select batches randomly\n",
    "            batch_size = batch_size \n",
    "        )\n",
    "\n",
    "# For validation the order doesn't really matter, \n",
    "# so DataLoader will just read them sequentially.\n",
    "validation_dataloader = DataLoader(\n",
    "            valid_dataset, # The validation samples.\n",
    "            sampler = SequentialSampler(valid_dataset), # Pull out batches sequentially.\n",
    "            batch_size = batch_size # Evaluate with this batch size.\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "07a3a970882744149b28d152bd46c4bf",
      "6a180cd9bf1d4316a87a0f2d5ef87524",
      "22f0c90692bf4d259fdebc90e4f7a025",
      "7621b73b1e274dea8ea6a1ead4521602",
      "616e907ff569442890ef3d40b6e6eda0",
      "e1f81fe2b18e4ae88c015b4ca29ab3ef",
      "eae687ef93754adc8dbc9541b6f889a2",
      "17812fbf01704bfd907fce632ecc3de7",
      "b508be1c7bda44f7b4a25608e50bc6af",
      "9c45b20296994786895cdc76238f172e",
      "473028919689435c8c49941d5dcb4f16",
      "db19f0da8d084ba0958b548cf2063a3f",
      "33e6130dd884490ea34e2e8499b480f7",
      "d320195e72d849b396839bb179a800d2",
      "950137ef014f43c399437c7a60f301a2",
      "1d6deab9db5b488ab9375d70cbd886ab"
     ]
    },
    "id": "LnFcjCUusmbS",
    "outputId": "e7d451a4-c0e2-470b-d823-f2556d75d411"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load BertForSequenceClassification from_pretrained()\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-uncased\", # Use the 12-layer BERT model, with an uncased vocab.\n",
    "    num_labels = 2, # Number of output labels--2 for binary classification (1 and 0).\n",
    "    output_attentions = False, # Whether the model returns attentions weights.\n",
    "    output_hidden_states = False, # Whether the model returns all hidden-states.\n",
    ")\n",
    "\n",
    "# Run this model on the GPU.\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "EyyIWTSdssxV"
   },
   "outputs": [],
   "source": [
    "# Initialize the AdamW optimizer \n",
    "\n",
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr = 2e-5,\n",
    "                  eps = 1e-8\n",
    "                )\n",
    "\n",
    "# Number of training epochs.\n",
    "epochs = 4\n",
    "\n",
    "# Total number of training steps is [number of batches] x [number of epochs]. \n",
    "total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "# Create the learning rate scheduler.\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps = 0,\n",
    "                                            num_training_steps = total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "RADEZz4RstWZ"
   },
   "outputs": [],
   "source": [
    "# Function to calculate the accuracy of our predictions vs labels\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
    "\n",
    "# I will use this function to know how much time the model needed to train\n",
    "def format_time(elapsed):\n",
    "    '''\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    '''\n",
    "    # Round to the nearest second.\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    \n",
    "    # Format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mmxUDnoTsxk2",
    "outputId": "5c039fb2-4fa9-4514-c14c-b95e5ecd750d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      "  Batch    40  of    217.    Elapsed: 0:00:06.\n",
      "  Batch    80  of    217.    Elapsed: 0:00:12.\n",
      "  Batch   120  of    217.    Elapsed: 0:00:18.\n",
      "  Batch   160  of    217.    Elapsed: 0:00:24.\n",
      "  Batch   200  of    217.    Elapsed: 0:00:30.\n",
      "\n",
      "  Average training loss: 0.26\n",
      "  Training epcoh took: 0:00:32\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.87\n",
      "  Validation Loss: 0.38\n",
      "  Validation took  0:00:01 time\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      "  Batch    40  of    217.    Elapsed: 0:00:06.\n",
      "  Batch    80  of    217.    Elapsed: 0:00:12.\n",
      "  Batch   120  of    217.    Elapsed: 0:00:18.\n",
      "  Batch   160  of    217.    Elapsed: 0:00:24.\n",
      "  Batch   200  of    217.    Elapsed: 0:00:30.\n",
      "\n",
      "  Average training loss: 0.08\n",
      "  Training epcoh took: 0:00:33\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.90\n",
      "  Validation Loss: 0.36\n",
      "  Validation took  0:00:01 time\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      "  Batch    40  of    217.    Elapsed: 0:00:06.\n",
      "  Batch    80  of    217.    Elapsed: 0:00:12.\n",
      "  Batch   120  of    217.    Elapsed: 0:00:18.\n",
      "  Batch   160  of    217.    Elapsed: 0:00:25.\n",
      "  Batch   200  of    217.    Elapsed: 0:00:31.\n",
      "\n",
      "  Average training loss: 0.03\n",
      "  Training epcoh took: 0:00:33\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.89\n",
      "  Validation Loss: 0.53\n",
      "  Validation took  0:00:01 time\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      "  Batch    40  of    217.    Elapsed: 0:00:06.\n",
      "  Batch    80  of    217.    Elapsed: 0:00:13.\n",
      "  Batch   120  of    217.    Elapsed: 0:00:19.\n",
      "  Batch   160  of    217.    Elapsed: 0:00:26.\n",
      "  Batch   200  of    217.    Elapsed: 0:00:32.\n",
      "\n",
      "  Average training loss: 0.02\n",
      "  Training epcoh took: 0:00:35\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.89\n",
      "  Validation Loss: 0.56\n",
      "  Validation took  0:00:01 time\n",
      "\n",
      "Training complete!\n",
      "Total training took 0:02:19 (h:mm:ss)\n"
     ]
    }
   ],
   "source": [
    "# We'll store a number of quantities such as training and validation loss, \n",
    "# validation accuracy, and timings.\n",
    "training_stats = []\n",
    "\n",
    "# Measure the total training time for the whole run.\n",
    "total_t0 = time.time()\n",
    "\n",
    "# For each epoch...\n",
    "for epoch_i in range(0, epochs):\n",
    "    \n",
    "    # ========================================\n",
    "    #               Training\n",
    "    # ========================================\n",
    "    \n",
    "    # Perform one full pass over the training set.\n",
    "\n",
    "    print(\"\")\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "    print('Training...')\n",
    "\n",
    "    # Measure how long the training epoch takes.\n",
    "    t0 = time.time()\n",
    "\n",
    "    # Reset the total loss for this epoch.\n",
    "    total_train_loss = 0\n",
    "\n",
    "    # Put the model into training mode. Don't be mislead--the call to \n",
    "    # `train` just changes the *mode*, it doesn't *perform* the training.\n",
    "    # `dropout` and `batchnorm` layers behave differently during training\n",
    "    # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
    "    model.train()\n",
    "\n",
    "    # For each batch of training data...\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "\n",
    "        # Progress update every 40 batches.\n",
    "        if step % 40 == 0 and not step == 0:\n",
    "            # Calculate elapsed time in minutes.\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "            \n",
    "            # Report progress.\n",
    "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
    "\n",
    "        # Unpack this training batch from our dataloader. \n",
    "        #\n",
    "        # As we unpack the batch, we'll also copy each tensor to the GPU using the \n",
    "        # `to` method.\n",
    "        #\n",
    "        # `batch` contains three pytorch tensors:\n",
    "        #   [0]: input ids \n",
    "        #   [1]: attention masks\n",
    "        #   [2]: labels \n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "\n",
    "        # Always clear any previously calculated gradients before performing a\n",
    "        # backward pass. PyTorch doesn't do this automatically because \n",
    "        # accumulating the gradients is \"convenient while training RNNs\". \n",
    "        # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
    "        model.zero_grad()        \n",
    "\n",
    "        # Perform a forward pass (evaluate the model on this training batch).\n",
    "        # The documentation for this `model` function is here: \n",
    "        # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
    "        # It returns different numbers of parameters depending on what arguments\n",
    "        # arge given and what flags are set. For our useage here, it returns\n",
    "        # the loss (because we provided labels) and the \"logits\"--the model\n",
    "        # outputs prior to activation.\n",
    "        loss, logits = model(b_input_ids, \n",
    "                             token_type_ids=None, \n",
    "                             attention_mask=b_input_mask, \n",
    "                             labels=b_labels, \n",
    "                             return_dict=False)\n",
    "\n",
    "\n",
    "        # Perform a backward pass to calculate the gradients.\n",
    "        loss.backward()\n",
    "\n",
    "        # Clip the norm of the gradients to 1.0.\n",
    "        # This is to help prevent the \"exploding gradients\" problem.\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        # Update parameters and take a step using the computed gradient.\n",
    "        # The optimizer dictates the \"update rule\"--how the parameters are\n",
    "        # modified based on their gradients, the learning rate, etc.\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "        # Accumulate the training loss over all of the batches so that we can\n",
    "        # calculate the average loss at the end. `loss` is a Tensor containing a\n",
    "        # single value; the `.item()` function just returns the Python value \n",
    "        # from the tensor.\n",
    "        #print(type(loss))\n",
    "        #print(loss)\n",
    "\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "        # Update the learning rate.\n",
    "        scheduler.step()\n",
    "\n",
    "    # Calculate the average loss over all of the batches.\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)            \n",
    "    \n",
    "    # Measure how long this epoch took.\n",
    "    training_time = format_time(time.time() - t0)\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "    print(\"  Training epcoh took: {:}\".format(training_time))\n",
    "        \n",
    "    # ========================================\n",
    "    #               Validation\n",
    "    # ========================================\n",
    "    # After the completion of each training epoch, measure our performance on\n",
    "    # our validation set.\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"Running Validation...\")\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    # Put the model in evaluation mode--the dropout layers behave differently\n",
    "    # during evaluation.\n",
    "    model.eval()\n",
    "\n",
    "    # Tracking variables \n",
    "    total_eval_accuracy = 0\n",
    "    total_eval_loss = 0\n",
    "    nb_eval_steps = 0\n",
    "\n",
    "    # Evaluate data for one epoch\n",
    "    for batch in validation_dataloader:\n",
    "        \n",
    "        # Unpack this training batch from our dataloader. \n",
    "        # As we unpack the batch, I'll also copy each tensor to the GPU using the `to` method.\n",
    "        #\n",
    "        # `batch` contains three pytorch tensors:\n",
    "        #   [0]: input ids \n",
    "        #   [1]: attention masks\n",
    "        #   [2]: labels \n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "        \n",
    "        # Tell pytorch not to bother with constructing the compute graph during\n",
    "        # the forward pass, since this is only needed for backprop (training).\n",
    "        with torch.no_grad():        \n",
    "\n",
    "            # Forward pass, calculate logit predictions.\n",
    "            # token_type_ids is the same as the \"segment ids\", which \n",
    "            # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
    "            # Get the \"logits\" output by the model. The \"logits\" are the output\n",
    "            # values prior to applying an activation function like the softmax.\n",
    "            (loss, logits) = model(b_input_ids, \n",
    "                                   token_type_ids=None, \n",
    "                                   attention_mask=b_input_mask,\n",
    "                                   labels=b_labels,\n",
    "                                   return_dict=False)\n",
    "            \n",
    "        # Accumulate the validation loss.\n",
    "        total_eval_loss += loss.item()\n",
    "\n",
    "        # Move logits and labels to CPU\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "        # Calculate the accuracy for this batch of test sentences, and\n",
    "        # accumulate it over all batches.\n",
    "        total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
    "        \n",
    "\n",
    "    # Report the final accuracy for this validation run.\n",
    "    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
    "    print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
    "\n",
    "    # Calculate the average loss over all of the batches.\n",
    "    avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
    "    \n",
    "    # Measure how long the validation run took.\n",
    "    validation_time = format_time(time.time() - t0)\n",
    "    \n",
    "    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
    "    print(\"  Validation took  {:} time\".format(validation_time))\n",
    "\n",
    "    # Record all statistics from this epoch.\n",
    "    training_stats.append(\n",
    "        {\n",
    "            'epoch': epoch_i + 1,\n",
    "            'Training Loss': avg_train_loss,\n",
    "            'Valid. Loss': avg_val_loss,\n",
    "            'Valid. Accur.': avg_val_accuracy,\n",
    "            'Training Time': training_time,\n",
    "            'Validation Time': validation_time\n",
    "        }\n",
    "    )\n",
    "\n",
    "print(\"\")\n",
    "print(\"Training complete!\")\n",
    "\n",
    "print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "Q45_sXoDs9Lx",
    "outputId": "ddc66ec0-96c9-418a-c569-15d0eb4c6125"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Valid. Loss</th>\n",
       "      <th>Valid. Accur.</th>\n",
       "      <th>Training Time</th>\n",
       "      <th>Validation Time</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>epoch</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.26</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0:00:32</td>\n",
       "      <td>0:00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.08</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0:00:33</td>\n",
       "      <td>0:00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0:00:33</td>\n",
       "      <td>0:00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.02</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0:00:35</td>\n",
       "      <td>0:00:01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Training Loss  Valid. Loss  Valid. Accur. Training Time Validation Time\n",
       "epoch                                                                         \n",
       "1               0.26         0.38           0.87       0:00:32         0:00:01\n",
       "2               0.08         0.36           0.90       0:00:33         0:00:01\n",
       "3               0.03         0.53           0.89       0:00:33         0:00:01\n",
       "4               0.02         0.56           0.89       0:00:35         0:00:01"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display floats with two decimal places.\n",
    "pd.set_option('precision', 2)\n",
    "\n",
    "# Create a DataFrame from our training statistics.\n",
    "df_statistics = pd.DataFrame(data=training_stats)\n",
    "\n",
    "# Use the 'epoch' as the row index.\n",
    "df_statistics = df_statistics.set_index('epoch')\n",
    "\n",
    "# A hack to force the column headers to wrap.\n",
    "#df = df.style.set_table_styles([dict(selector=\"th\",props=[('max-width', '70px')])])\n",
    "\n",
    "# Display the table.\n",
    "df_statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 427
    },
    "id": "cyGdQzKItEbl",
    "outputId": "202c0991-f7f2-46c0-8aca-8cfd546393bd"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Use plot styling from seaborn.\n",
    "sns.set(style='darkgrid')\n",
    "\n",
    "# Increase the plot size and font size.\n",
    "sns.set(font_scale=1.5)\n",
    "plt.rcParams[\"figure.figsize\"] = (12,6)\n",
    "\n",
    "# Plot the learning curve.\n",
    "plt.plot(df_statistics['Training Loss'], 'b-o', label=\"Training\")\n",
    "plt.plot(df_statistics['Valid. Loss'], 'g-o', label=\"Validation\")\n",
    "\n",
    "# Label the plot.\n",
    "plt.title(\"Training & Validation Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.xticks([1, 2, 3, 4])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0Skua029tIfC",
    "outputId": "b32dbec4-63ac-40f7-da9a-1a8850e5aa42"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of test sentences: 2,000\n",
      "\n",
      "test Original sentence:  @user pa hospitals don’t have the capacity. latest from our local hospital is they will only test those with symptoms who returned from china, italy, iran, japan , or south korea in last 14 days or those who have been in contact with someone with confirmed covid-19 in the last 14 days.\n",
      "test Token IDs list: tensor([  101,  1030,  5310,  6643,  8323,  2123,  1521,  1056,  2031,  1996,\n",
      "         3977,  1012,  6745,  2013,  2256,  2334,  2902,  2003,  2027,  2097,\n",
      "         2069,  3231,  2216,  2007,  8030,  2040,  2513,  2013,  2859,  1010,\n",
      "         3304,  1010,  4238,  1010,  2900,  1010,  2030,  2148,  4420,  1999,\n",
      "         2197,  2403,  2420,  2030,  2216,  2040,  2031,  2042,  1999,  3967,\n",
      "         2007,  2619,  2007,  4484,  2522, 17258,  1011,  2539,  1999,  1996,\n",
      "         2197,  2403,  2420,   102])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/boney/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2068: FutureWarning:\n",
      "\n",
      "The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Report the number of sentences.\n",
    "print('Number of test sentences: {:,}\\n'.format(test.shape[0]))\n",
    "\n",
    "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
    "test_input_ids = []\n",
    "test_attention_masks = []\n",
    "\n",
    "# For every sentence...\n",
    "for sent in test_sentences:\n",
    "    # `encode_plus` will:\n",
    "    #   (1) Tokenize the sentence.\n",
    "    #   (2) Prepend the `[CLS]` token to the start.\n",
    "    #   (3) Append the `[SEP]` token to the end.\n",
    "    #   (4) Map tokens to their IDs.\n",
    "    #   (5) Pad or truncate the sentence to `max_length`\n",
    "    #   (6) Create attention masks for [PAD] tokens.\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "                        sent,                      # Sentence to encode.\n",
    "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                        max_length = 64,           # Pad & truncate all sentences.\n",
    "                        pad_to_max_length = True,\n",
    "                        return_attention_mask = True,   # Construct attn. masks.\n",
    "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                   )\n",
    "    \n",
    "    # Add the encoded sentence to the list.    \n",
    "    test_input_ids.append(encoded_dict['input_ids'])\n",
    "    \n",
    "    # And its attention mask (simply differentiates padding from non-padding).\n",
    "    test_attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "# Convert the lists into tensors.\n",
    "test_input_ids = torch.cat(test_input_ids, dim=0)\n",
    "test_attention_masks = torch.cat(test_attention_masks, dim=0)\n",
    "\n",
    "test_labels = torch.tensor(test_labels)\n",
    "\n",
    "# Print sentence 0, now as a list of IDs.\n",
    "print('test Original sentence: ', test_sentences[0])\n",
    "print('test Token IDs list:', test_input_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "lWSZfvNgtOW7"
   },
   "outputs": [],
   "source": [
    "# Set test_data_set, the batch size and create DataLoader\n",
    "test_dataset = TensorDataset(test_input_ids, test_attention_masks, test_labels)\n",
    "\n",
    "batch_size = 32 # same as in training: 32  \n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "            test_dataset, # The test samples.\n",
    "            sampler = SequentialSampler(test_dataset), # Pull out batches sequentially.\n",
    "            batch_size = batch_size # Evaluate with this batch size.\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JtNDwDMMtclK",
    "outputId": "d9c437a6-6089-4827-d2e8-957aa639f1ca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting labels for 2,000 test sentences...\n",
      "  Accuracy: 0.88\n",
      "  Test Loss: 0.61\n",
      "  Test took  0:00:03 time\n"
     ]
    }
   ],
   "source": [
    "    print(\"\")\n",
    "    print('Predicting labels for {:,} test sentences...'.format(len(test_input_ids)))\n",
    "\n",
    "    test_stats = []\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    # Put the model in evaluation mode--the dropout layers behave differently\n",
    "    # during evaluation.\n",
    "    model.eval()\n",
    "\n",
    "    # Tracking variables \n",
    "    total_eval_accuracy = 0\n",
    "    total_eval_loss = 0\n",
    "    nb_eval_steps = 0\n",
    "\n",
    "    # Evaluate data for one epoch\n",
    "    for batch in test_dataloader:\n",
    "        \n",
    "        # Unpack this training batch from our dataloader. \n",
    "        # As we unpack the batch, I'll also copy each tensor to the GPU using the `to` method.\n",
    "        #\n",
    "        # `batch` contains three pytorch tensors:\n",
    "        #   [0]: input ids \n",
    "        #   [1]: attention masks\n",
    "        #   [2]: labels \n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "        \n",
    "        # Tell pytorch not to bother with constructing the compute graph during\n",
    "        # the forward pass, since this is only needed for backprop (training).\n",
    "        with torch.no_grad():        \n",
    "\n",
    "            # Forward pass, calculate logit predictions.\n",
    "            # token_type_ids is the same as the \"segment ids\", which \n",
    "            # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
    "            # Get the \"logits\" output by the model. The \"logits\" are the output\n",
    "            # values prior to applying an activation function like the softmax.\n",
    "            (loss, logits) = model(b_input_ids, \n",
    "                                   token_type_ids=None, \n",
    "                                   attention_mask=b_input_mask,\n",
    "                                   labels=b_labels,\n",
    "                                   return_dict=False)\n",
    "            \n",
    "        # Accumulate the validation loss.\n",
    "        total_eval_loss += loss.item()\n",
    "\n",
    "        # Move logits and labels to CPU\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "        # Calculate the accuracy for this batch of test sentences, and\n",
    "        # accumulate it over all batches.\n",
    "        total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
    "        \n",
    "\n",
    "    # Report the final accuracy for this validation run.\n",
    "    avg_test_accuracy = total_eval_accuracy / len(test_dataloader)\n",
    "    print(\"  Accuracy: {0:.2f}\".format(avg_test_accuracy))\n",
    "\n",
    "    # Calculate the average loss over all of the batches.\n",
    "    avg_test_loss = total_eval_loss / len(test_dataloader)\n",
    "    \n",
    "    # Measure how long the validation run took.\n",
    "    test_time = format_time(time.time() - t0)\n",
    "    \n",
    "    print(\"  Test Loss: {0:.2f}\".format(avg_test_loss))\n",
    "    print(\"  Test took  {:} time\".format(test_time))\n",
    "\n",
    "    # Record all statistics from this epoch.\n",
    "    test_stats.append(\n",
    "        {\n",
    "            'Test. Loss': avg_val_loss,\n",
    "            'Test. Accur.': avg_val_accuracy,\n",
    "            'Test time': test_time\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyO7aWgfAgGTb1y4azY/t2qz",
   "machine_shape": "hm",
   "mount_file_id": "1vPl_-6wi6D35pI7aYYLjYm0oYl2y88n-",
   "name": "DL_NLP_Project_COVID-19_BERT4SC.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "026ac54a29574a7f87bc31153572934d": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6f3874a4359d4cd08a174b4e9d71c360",
      "placeholder": "​",
      "style": "IPY_MODEL_2e6cb02d5ed44204b83a639cbc75ada1",
      "value": " 28.0/28.0 [00:00&lt;00:00, 35.3B/s]"
     }
    },
    "06facbbcca6b47e5b50a92aff13ef54c": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "07a3a970882744149b28d152bd46c4bf": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_22f0c90692bf4d259fdebc90e4f7a025",
       "IPY_MODEL_7621b73b1e274dea8ea6a1ead4521602"
      ],
      "layout": "IPY_MODEL_6a180cd9bf1d4316a87a0f2d5ef87524"
     }
    },
    "0a08d24cdddb4f248d62bf08499b7e2d": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_46a47186f0074ca9949b98927e61f4f5",
       "IPY_MODEL_026ac54a29574a7f87bc31153572934d"
      ],
      "layout": "IPY_MODEL_a911c1406be3402997126058e34a39dd"
     }
    },
    "154c6f4caee349919a49d8d4a2ddf266": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "17812fbf01704bfd907fce632ecc3de7": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1d6deab9db5b488ab9375d70cbd886ab": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "22f0c90692bf4d259fdebc90e4f7a025": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e1f81fe2b18e4ae88c015b4ca29ab3ef",
      "max": 570,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_616e907ff569442890ef3d40b6e6eda0",
      "value": 570
     }
    },
    "24f77f432f7b4e14a7947cd03c798dd9": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2e6cb02d5ed44204b83a639cbc75ada1": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "2f308c8f2b4c452f899d0f29a59b6b49": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_154c6f4caee349919a49d8d4a2ddf266",
      "placeholder": "​",
      "style": "IPY_MODEL_a0a46ac119ee4b439e8c6662122faad5",
      "value": " 466k/466k [00:00&lt;00:00, 1.36MB/s]"
     }
    },
    "33e6130dd884490ea34e2e8499b480f7": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "38c3dbee3b8940379d8f48e0f88dfd59": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3c5e113589ab4f949707a1539b1f9af0": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "46a47186f0074ca9949b98927e61f4f5": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_dd52bd4a2d184559a0f4e50eb81107b6",
      "max": 28,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_750e8382dd814da39e1294a57c4d1e7a",
      "value": 28
     }
    },
    "473028919689435c8c49941d5dcb4f16": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d320195e72d849b396839bb179a800d2",
      "max": 440473133,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_33e6130dd884490ea34e2e8499b480f7",
      "value": 440473133
     }
    },
    "4b4d0b221e3a49dd8dad21974d5fa44a": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_38c3dbee3b8940379d8f48e0f88dfd59",
      "max": 231508,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_4cf1975862094034b527fcd5c8d67be2",
      "value": 231508
     }
    },
    "4cf1975862094034b527fcd5c8d67be2": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "4eea6d5c9d794e6dbd1489b97bce5a42": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_06facbbcca6b47e5b50a92aff13ef54c",
      "max": 466062,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_8e4ae2ff85554d7d95db3f3571578f2c",
      "value": 466062
     }
    },
    "616e907ff569442890ef3d40b6e6eda0": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "6a180cd9bf1d4316a87a0f2d5ef87524": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6f3874a4359d4cd08a174b4e9d71c360": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "750e8382dd814da39e1294a57c4d1e7a": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "7621b73b1e274dea8ea6a1ead4521602": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_17812fbf01704bfd907fce632ecc3de7",
      "placeholder": "​",
      "style": "IPY_MODEL_eae687ef93754adc8dbc9541b6f889a2",
      "value": " 570/570 [00:23&lt;00:00, 24.2B/s]"
     }
    },
    "854738ab2edd485b97a2b0be4c4e3215": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_4b4d0b221e3a49dd8dad21974d5fa44a",
       "IPY_MODEL_a3076b34cf04458f8012359ce912d8a8"
      ],
      "layout": "IPY_MODEL_cfdbb6ec3b004d1c9733c25d52c777ba"
     }
    },
    "8e4ae2ff85554d7d95db3f3571578f2c": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "949bfeae43a847d3a0de2a40f3ed5579": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "950137ef014f43c399437c7a60f301a2": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "9c45b20296994786895cdc76238f172e": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a0a46ac119ee4b439e8c6662122faad5": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a3076b34cf04458f8012359ce912d8a8": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_24f77f432f7b4e14a7947cd03c798dd9",
      "placeholder": "​",
      "style": "IPY_MODEL_949bfeae43a847d3a0de2a40f3ed5579",
      "value": " 232k/232k [00:01&lt;00:00, 122kB/s]"
     }
    },
    "a911c1406be3402997126058e34a39dd": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b508be1c7bda44f7b4a25608e50bc6af": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_473028919689435c8c49941d5dcb4f16",
       "IPY_MODEL_db19f0da8d084ba0958b548cf2063a3f"
      ],
      "layout": "IPY_MODEL_9c45b20296994786895cdc76238f172e"
     }
    },
    "cfdbb6ec3b004d1c9733c25d52c777ba": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d320195e72d849b396839bb179a800d2": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "db19f0da8d084ba0958b548cf2063a3f": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1d6deab9db5b488ab9375d70cbd886ab",
      "placeholder": "​",
      "style": "IPY_MODEL_950137ef014f43c399437c7a60f301a2",
      "value": " 440M/440M [00:11&lt;00:00, 37.8MB/s]"
     }
    },
    "dd52bd4a2d184559a0f4e50eb81107b6": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e1f81fe2b18e4ae88c015b4ca29ab3ef": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "eae687ef93754adc8dbc9541b6f889a2": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f9751896cf88402dbad23aa403a94a6c": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_4eea6d5c9d794e6dbd1489b97bce5a42",
       "IPY_MODEL_2f308c8f2b4c452f899d0f29a59b6b49"
      ],
      "layout": "IPY_MODEL_3c5e113589ab4f949707a1539b1f9af0"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
